"","title","author","subject","abstract","meta"
"1","Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization","Ratnadira Widyasari, Jia Wei Ang, Truong Giang Nguyen, Neil Sharma, David Lo","Software Engineering (cs.SE)","Fault localization is a critical process that involves identifying specific program elements responsible for program failures. Manually pinpointing these elements, such as classes, methods, or statements, which are associated with a fault is laborious and time-consuming. To overcome this challenge, various fault localization tools have been developed. These tools typically generate a ranked list of suspicious program elements. However, this information alone is insufficient. A prior study emphasized that automated fault localization should offer a rationale. In this study, we investigate the step-by-step reasoning for explainable fault localization. We explore the potential of Large Language Models (LLM) in assisting developers in reasoning about code. We proposed FuseFL that utilizes several combinations of information to enhance the LLM results which are spectrum-based fault localization results, test case execution outcomes, and code description (i.e., explanation of what the given code is intended to do). We conducted our investigation using faulty code from Refactory dataset. First, we evaluate the performance of the automated fault localization. Our results demonstrate a more than 30% increase in the number of successfully localized faults at Top-1 compared to the baseline. To evaluate the explanations generated by FuseFL, we create a dataset of human explanations that provide step-by-step reasoning as to why specific lines of code are considered faulty. This dataset consists of 324 faulty code files, along with explanations for 600 faulty lines. Furthermore, we also conducted human studies to evaluate the explanations. We found that for 22 out of the 30 randomly sampled cases, FuseFL generated correct explanations.","Fri, 15 Mar 2024 17:47:20 UTC (518 KB)"
"2","ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment","Xiaofeng Wu, Jia Rao, Wei Chen","Distributed, Parallel, and Cluster Computing (cs.DC)","The advent of the Transformer architecture has propelled the growth of natural language processing (NLP) models, leading to remarkable achievements in numerous NLP tasks. Yet, the absence of specialized hardware like expansive GPU memory and high-speed interconnects poses challenges for training large-scale models. This makes it daunting for many users to experiment with pre-training and fine-tuning large language models (LLMs). In this study, we introduce \atom, a resilient distributed training framework designed for asynchronous training of vast models in a decentralized setting using cost-effective hardware, including consumer-grade GPUs and Ethernet. Unlike conventional model partitioning methods that distribute sub-models across GPUs, \atom aims to accommodate a complete LLM on one host (peer) through seamlessly model swapping and concurrently trains multiple copies across various peers to optimize training throughput. Through static analysis, \atom identifies the best model partitioning strategy and flawlessly merges model execution with swapping. Key benefits of \atom include: Avoiding the central point of failure found in pipeline parallelism methods. Demonstrating superior performance and scalability compared to closely-integrated pipeline parallelism in slower networks. Our experiments using different GPT-3 model configurations reveal that, in scenarios with suboptimal network connections, \atom can enhance training efficiency up to $20 \times$ when juxtaposed with the state-of-the-art decentralized pipeline parallelism approaches.","Fri, 15 Mar 2024 17:43:43 UTC (1,042 KB)"
"3","An Empirical Study on Developers Shared Conversations with ChatGPT in GitHub Pull Requests and Issues","Huizi Hao, Kazi Amit Hasan, Hong Qin, Marcos Macedo, Yuan Tian, Steven H. H. Ding, Ahmed E. Hassan","Software Engineering (cs.SE)","ChatGPT has significantly impacted software development practices, providing substantial assistance to developers in a variety of tasks, including coding, testing, and debugging. Despite its widespread adoption, the impact of ChatGPT as an assistant in collaborative coding remains largely unexplored. In this paper, we analyze a dataset of 210 and 370 developers shared conversations with ChatGPT in GitHub pull requests (PRs) and issues. We manually examined the content of the conversations and characterized the dynamics of the sharing behavior, i.e., understanding the rationale behind the sharing, identifying the locations where the conversations were shared, and determining the roles of the developers who shared them. Our main observations are: (1) Developers seek ChatGPT assistance across 16 types of software engineering inquiries. In both conversations shared in PRs and issues, the most frequently encountered inquiry categories include code generation, conceptual questions, how-to guides, issue resolution, and code review. (2) Developers frequently engage with ChatGPT via multi-turn conversations where each prompt can fulfill various roles, such as unveiling initial or new tasks, iterative follow-up, and prompt refinement. Multi-turn conversations account for 33.2% of the conversations shared in PRs and 36.9% in issues. (3) In collaborative coding, developers leverage shared conversations with ChatGPT to facilitate their role-specific contributions, whether as authors of PRs or issues, code reviewers, or collaborators on issues. Our work serves as the first step towards understanding the dynamics between developers and ChatGPT in collaborative software development and opens up new directions for future research on the topic.","Fri, 15 Mar 2024 16:58:37 UTC (3,498 KB)"
"4","pyCEPS: A cross-platform Electroanatomic Mapping Data to Computational Model Conversion Platform for the Calibration of Digital Twin Models of Cardiac Electrophysiology","Robert Arnold, Anton J. Prassl, Aurel Neic, Franz Thaler, Christoph M. Augustin, Matthias A. F. Gsell, Karli Gillette, Martin Manninger, Daniel Scherr, Gernot Plank","Medical Physics (physics.med-ph)","Background and Objective: Data from electro-anatomical mapping (EAM) systems are playing an increasingly important role in computational modeling studies for the patient-specific calibration of digital twin models. However, data exported from commercial EAM systems are challenging to access and parse. Converting to data formats that are easily amenable to be viewed and analyzed with commonly used cardiac simulation software tools such as openCARP remains challenging. We therefore developed an open-source platform, pyCEPS, for parsing and converting clinical EAM data conveniently to standard formats widely adopted within the cardiac modeling community. Methods and Results: pyCEPS is an open-source Python-based platform providing the following functions: (i) access and interrogate the EAM data exported from clinical mapping systems; (ii) efficient browsing of EAM data to preview mapping procedures, electrograms (EGMs), and electro-cardiograms (ECGs); (iii) conversion to modeling formats according to the openCARP standard, to be amenable to analysis with standard tools and advanced workflows as used for in silico EAM data. Documentation and training material to facilitate access to this complementary research tool for new users is provided. We describe the technological underpinnings and demonstrate the capabilities of pyCEPS first, and showcase its use in an exemplary modeling application where we use clinical imaging data to build a patient-specific anatomical model. Conclusion: With pyCEPS we offer an open-source framework for accessing EAM data, and converting these to cardiac modeling standard formats. pyCEPS provides the core functionality needed to integrate EAM data in cardiac modeling research. We detail how pyCEPS could be integrated into model calibration workflows facilitating the calibration of a computational model based on EAM data.","Fri, 15 Mar 2024 15:27:57 UTC (22,921 KB)"
"5","Application of machine learning to experimental design in quantum mechanics","Federico Belliardo, Fabio Zoratti, Vittorio Giovannetti","Quantum Physics (quant-ph)","The recent advances in machine learning hold great promise for the fields of quantum sensing and metrology. With the help of reinforcement learning, we can tame the complexity of quantum systems and solve the problem of optimal experimental design. Reinforcement learning is a powerful model-free technique that allows an agent, typically a neural network, to learn the best strategy to reach a certain goal in a completely a priori unknown environment. However, in general, we know something about the quantum system with which the agent is interacting, at least that it follows the rules of quantum mechanics. In quantum metrology, we typically have a model for the system, and only some parameters of the evolution or the initial state are unknown. We present here a general machine learning technique that can optimize the precision of quantum sensors, exploiting the knowledge we have on the system through model-aware reinforcement learning. This framework has been implemented in the Python package qsensoropt, which is able to optimize a broad class of problems found in quantum metrology and quantum parameter estimation. The agent learns an optimal adaptive strategy that, based on previous outcomes, decides the next measurements to perform. We have explored some applications of this technique to NV centers and photonic circuits. So far, we have been able to certify better results than the current state-of-the-art controls for many cases. The machine learning technique developed here can be applied in all scenarios where the quantum system is well-characterized and relatively simple and small. In these cases, we can extract every last bit of information from a quantum sensor by appropriately controlling it with a trained neural network. The qsensoropt software is available on PyPI and can be installed with pip.","Fri, 15 Mar 2024 14:07:46 UTC (456 KB)"
"6","Formal Security Analysis of the AMD SEV-SNP Software Interface","Petar Paradžik, Ante Derek, Marko Horvat","Cryptography and Security (cs.CR)","AMD Secure Encrypted Virtualization technologies enable confidential computing by protecting virtual machines from highly privileged software such as hypervisors. In this work, we develop the first, comprehensive symbolic model of the software interface of the latest SEV iteration called SEV Secure Nested Paging (SEV-SNP). Our model covers remote attestation, key derivation, page swap and live migration. We analyze the security of the software interface of SEV-SNP by verifying critical secrecy, authentication, attestation and freshness properties, and find that the platform-agnostic nature of messages exchanged between SNP guests and the AMD Secure Processor firmware presents a weakness of the design. We show multiple ways of exploiting this weakness, including the compromise of attestation report integrity, and suggest slight modifications to the design which let third parties detect guest migrations to vulnerable platforms","Fri, 15 Mar 2024 13:39:55 UTC (1,672 KB)"
"7","A Conceptual Model for the Analysis of Investigation Elements in Games","Pedro Marques, Marcus Parreiras, Joshua Kritz, Geraldo Xexeo","Software Engineering (cs.SE)","This paper presents the 4E conceptual model, developed to formally analyze investigation games from a game design perspective. The model encompasses four components: Exploration, Elicitation, Experimentation, and Evaluation. Grounded Theory was employed as the methodology for constructing the model, allowing for an in-depth understanding of the underlying concepts. The resulting model was then compared to existing literature, and its contributions were thoroughly discussed. Overall, the 4E model presents a comprehensive framework for understanding investigation games elements. It's application in two real-world scenarios demonstrates its practical relevance.","Fri, 15 Mar 2024 13:04:30 UTC (5,043 KB)"
"8","A Vocabulary of Board Game Dynamics","Joshua Kritz, Geraldo Xexéo","Software Engineering (cs.SE)","In recent years, significant advances have been made in the field of game research. However, there has been a noticeable dearth of scholarly research focused on the domain of dynamics, despite the widespread recognition among researchers of its existence and importance. The objective of this paper is to address this research gap by presenting a vocabulary dedicated to boardgame dynamics. To achieve this goal, we employ a focus group to generate a set of dynamic concepts that are subsequently subjected to validation and refinement through a survey. The resulting concepts are then organized into a vocabulary using a taxonomic structure, allowing the grouping of these concepts into broader and more general ideas.","Fri, 15 Mar 2024 12:56:10 UTC (259 KB)"
"9","Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties","Denis Schwachhofer, Peter Domanski, Steffen Becker, Stefan Wagner, Matthias Sauer, Dirk Pflüger, Ilia Polian","Software Engineering (cs.SE)","System-Level Test (SLT) has been a part of the test flow for integrated circuits for over a decade and still gains importance. However, no systematic approaches exist for test program generation, especially targeting non-functional properties of the Device under Test (DUT). Currently, test engineers manually compose test suites from off-the-shelf software, approximating the end-user environment of the DUT. This is a challenging and tedious task that does not guarantee sufficient control over non-functional properties. This paper proposes Large Language Models (LLMs) to generate test programs. We take a first glance at how pre-trained LLMs perform in test program generation to optimize non-functional properties of the DUT. Therefore, we write a prompt to generate C code snippets that maximize the instructions per cycle of a super-scalar, out-of-order architecture in simulation. Additionally, we apply prompt and hyperparameter optimization to achieve the best possible results without further training.","Fri, 15 Mar 2024 08:01:02 UTC (430 KB)"
"10","Repoformer: Selective Retrieval for Repository-Level Code Completion","Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, Xiaofei Ma","Software Engineering (cs.SE)","Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). To tackle the challenges, this paper proposes a selective RAG framework where retrieval is avoided when unnecessary. To power this framework, we design a self-supervised learning approach that enables a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective retrieval policy and the generation model, our framework consistently outperforms the state-of-the-art prompting with an invariable retrieval approach on diverse benchmarks including RepoEval, CrossCodeEval, and a new benchmark. Meanwhile, our selective retrieval strategy results in strong efficiency improvements by as much as 70% inference speedup without harming the performance. We demonstrate that our framework effectively accommodates different generation models, retrievers, and programming languages. These advancements position our framework as an important step towards more accurate and efficient repository-level code completion.","Fri, 15 Mar 2024 06:59:43 UTC (9,421 KB)"
"11","Ipelets for the Convex Polygonal Geometry","Nithin Parepally, Ainesh Chatterjee, Auguste Gezalyan, Hongyang Du, Sukrit Mangla, Kenny Wu, Sarah Hwang, David Mount","Computational Geometry (cs.CG)","There are many structures, both classical and modern, involving convex polygonal geometries whose deeper understanding would be facilitated through interactive visualizations. The Ipe extensible drawing editor, developed by Otfried Cheong, is a widely used software system for generating geometric figures. One of its features is the capability to extend its functionality through programs called Ipelets. In this media submission, we showcase a collection of new Ipelets that construct a variety of geometric objects based on polygonal geometries. These include Macbeath regions, metric balls in the forward and reverse Funk distance, metric balls in the Hilbert metric, polar bodies, the minimum enclosing ball of a point set, and minimum spanning trees in both the Funk and Hilbert metrics. We also include a number of utilities on convex polygons, including union, intersection, subtraction, and Minkowski sum (previously implemented as a CGAL Ipelet). All of our Ipelets are programmed in Lua and are freely available.","Fri, 15 Mar 2024 05:51:28 UTC (418 KB)"
"12","Securing Federated Learning with Control-Flow Attestation: A Novel Framework for Enhanced Integrity and Resilience against Adversarial Attacks","Zahir Alsulaimawi","Cryptography and Security (cs.CR)","The advent of Federated Learning (FL) as a distributed machine learning paradigm has introduced new cybersecurity challenges, notably adversarial attacks that threaten model integrity and participant privacy. This study proposes an innovative security framework inspired by Control-Flow Attestation (CFA) mechanisms, traditionally used in cybersecurity, to ensure software execution integrity. By integrating digital signatures and cryptographic hashing within the FL framework, we authenticate and verify the integrity of model updates across the network, effectively mitigating risks associated with model poisoning and adversarial interference. Our approach, novel in its application of CFA principles to FL, ensures contributions from participating nodes are authentic and untampered, thereby enhancing system resilience without compromising computational efficiency or model performance. Empirical evaluations on benchmark datasets, MNIST and CIFAR-10, demonstrate our framework's effectiveness, achieving a 100\% success rate in integrity verification and authentication and notable resilience against adversarial attacks. These results validate the proposed security enhancements and open avenues for more secure, reliable, and privacy-conscious distributed machine learning solutions. Our work bridges a critical gap between cybersecurity and distributed machine learning, offering a foundation for future advancements in secure FL.","Fri, 15 Mar 2024 04:03:34 UTC (488 KB)"
"13","Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models","Jiahui Wu, Chengjie Lu, Aitor Arrieta, Tao Yue, Shaukat Ali","Software Engineering (cs.SE)","Large Language Models (LLMs) are demonstrating outstanding potential for tasks such as text generation, summarization, and classification. Given that such models are trained on a humongous amount of online knowledge, we hypothesize that LLMs can assess whether driving scenarios generated by autonomous driving testing techniques are realistic, i.e., being aligned with real-world driving conditions. To test this hypothesis, we conducted an empirical evaluation to assess whether LLMs are effective and robust in performing the task. This reality check is an important step towards devising LLM-based autonomous driving testing techniques. For our empirical evaluation, we selected 64 realistic scenarios from \deepscenario--an open driving scenario dataset. Next, by introducing minor changes to them, we created 512 additional realistic scenarios, to form an overall dataset of 576 scenarios. With this dataset, we evaluated three LLMs (\gpt, \llama, and \mistral) to assess their robustness in assessing the realism of driving scenarios. Our results show that: (1) Overall, \gpt achieved the highest robustness compared to \llama and \mistral, consistently throughout almost all scenarios, roads, and weather conditions; (2) \mistral performed the worst consistently; (3) \llama achieved good results under certain conditions; and (4) roads and weather conditions do influence the robustness of the LLMs.","Thu, 14 Mar 2024 22:38:20 UTC (571 KB)"
"14","NN-Defined Modulator: Reconfigurable and Portable Software Modulator on IoT Gateways","Jiazhao Wang, Wenchao Jiang, Ruofeng Liu, Bin Hu, Demin Gao, Shuai Wang","Emerging Technologies (cs.ET)","A physical-layer modulator is a vital component for an IoT gateway to map the symbols to signals. However, due to the soldered hardware chipsets on the gateway's motherboards or the diverse toolkits on different platforms for the software radio, the existing solutions either have limited extensibility or are platform-specific. Such limitation is hard to ignore when modulation schemes and hardware platforms have become extremely diverse. This paper presents a new paradigm of using neural networks as an abstraction layer for physical layer modulators in IoT gateway devices, referred to as NN-defined modulators. Our approach addresses the challenges of extensibility and portability for multiple technologies on various hardware platforms. The proposed NN-defined modulator uses a model-driven methodology rooted in solid mathematical foundations while having native support for hardware acceleration and portability to heterogeneous platforms. We conduct the evaluation of NN-defined modulators on different platforms, including Nvidia Jetson Nano and Raspberry Pi. Evaluations demonstrate that our NN-defined modulator effectively operates as conventional modulators and provides significant efficiency gains (up to $4.7\times$ on Nvidia Jetson Nano and $1.1\times$ on Raspberry Pi), indicating high portability. Furthermore, we show the real-world applications using our NN-defined modulators to generate ZigBee and WiFi packets, which are compliant with commodity TI CC2650 (ZigBee) and Intel AX201 (WiFi NIC), respectively.","Thu, 14 Mar 2024 20:42:23 UTC (3,145 KB)"
"15","A Tale of Two Communities: Exploring Academic References on Stack Overflow","Run Huang, Souti Chattopadhyay","Computers and Society (cs.CY)","Stack Overflow is widely recognized by software practitioners as the go-to resource for addressing technical issues and sharing practical solutions. While it is not typically seen as a forum for scholarly discourse, users on Stack Overflow often refer to academic sources in their discussions. Yet, little is known about these referenced works from the academic community and how they intersect the needs and interests of the Stack Overflow community. To bridge this gap, we conducted a large-scale study on academic references in Stack Overflow. Our findings reveal that Stack Overflow communities with different domains of interest engage with academic literature at varying frequencies and speeds. The contradicting patterns suggest that some disciplines may have diverged in their interests and development trajectories from the corresponding practitioner community. Finally, we discuss the potential of Stack Overflow in gauging the real-world relevance of academic research.","Thu, 14 Mar 2024 20:33:55 UTC (2,431 KB)"
"16","Gamified GUI testing with Selenium in the IntelliJ IDE: A Prototype Plugin","Giacomo Garaccione, Tommaso Fulcini, Paolo Stefanut Bodnarescul, Riccardo Coppola, Luca Ardito","Software Engineering (cs.SE)","Software testing is a crucial phase in software development, enabling the detection of issues and defects that may arise during the development process. Addressing these issues enhances software applications' quality, reliability, user experience, and performance. Graphical User Interface (GUI) testing, one such technique, involves mimicking a regular user's interactions with an application to identify defects. However, GUI testing is often underutilized due to its perceived repetitiveness, error-proneness, and lack of immediate feedback on test quality. In recent years, gamification-incorporating game elements in non-game contexts to boost interest, motivation, and engagement-has gained traction in various fields, including software engineering and education. This paper presents GIPGUT: a prototype of a gamification plugin for IntelliJ IDEA, an Integrated Development Environment (IDE) that supports scripted GUI testing. The plugin enhances testers' engagement with typically monotonous and tedious tasks through achievements, rewards, and profile customization. A preliminary prototype evaluation was conducted with a small group of users to assess its usability and the impact of gamification on the GUI testing process. The results indicate high usability and positive reception of the gamification elements. However, due to the limited sample size of participants, further research is necessary to understand the plugin's effectiveness fully.","Thu, 14 Mar 2024 20:11:11 UTC (303 KB)"
"17","Teaching Machines to Code: Smart Contract Translation with LLMs","Rabimba Karanjai, Lei Xu, Weidong Shi","Software Engineering (cs.SE)","The advent of large language models (LLMs) has marked a significant milestone in the realm of artificial intelligence, with their capabilities often matching or surpassing human expertise in various domains. Among these achievements, their adeptness in translation tasks stands out, closely mimicking the intricate and preliminary processes undertaken by human translators to ensure the fidelity and quality of the translated content. Despite the advancements in utilizing LLMs for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the LLM, remains largely unexplored. In our research, we present a pioneering approach, SolMover, which harnesses the synergy of two distinct LLMs within a unified framework. This framework is designed to grasp coding principles and apply this understanding to the translation of code into an unfamiliar language. Our study delves into the capacity of LLMs to mimic human learning processes, offering an in-depth evaluation of our methodology for converting smart contracts written in Solidity to Move, a language with limited resources. The framework employs one LLM to decipher coding conventions for the new language, creating a blueprint for the second LLM, which, lacking planning abilities, possesses coding expertise. The empirical evidence from our experiments suggests that SolMover substantially enhances performance compared to gpt-3.5-turbo-1106, and achieves superior results over competitors such as Palm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the efficacy of our bug mitigation strategy in elevating code quality across all models, even outside the SolMover framework.","Wed, 13 Mar 2024 18:55:20 UTC (1,020 KB)"
"18","Textual analysis of End User License Agreement for red-flagging potentially malicious software","Behraj Khan, Tahir Syed, Zeshan Khan, Muhammad Rafi","Software Engineering (cs.SE)","New software and updates are downloaded by end users every day. Each dowloaded software has associated with it an End Users License Agreements (EULA), but this is rarely read. An EULA includes information to avoid legal repercussions. However,this proposes a host of potential problems such as spyware or producing an unwanted affect in the target system. End users do not read these EULA's because of length of the document and users find it extremely difficult to understand. Text summarization is one of the relevant solution to these kind of problems. This require a solution which can summarize the EULA and classify the EULA as ""Benign"" or ""Malicious"". We propose a solution in which we have summarize the EULA and classify the EULA as ""Benign"" or ""Malicious"". We extract EULA text of different sofware's then we classify the text using eight different supervised classifiers. we use ensemble learning to classify the EULA as benign or malicious using five different text summarization methods. An accuracy of $95.8$\% shows the effectiveness of the presented approach.","Mon, 11 Mar 2024 20:45:27 UTC (526 KB)"
"19","On locally symmetric polynomial metrics: Riemannian and Finslerian surfaces","Csaba Vincze, Márk Oláh, Ábris Nagy","Differential Geometry (math.DG)","In the paper we investigate locally symmetric polynomial metrics in special cases of Riemannian and Finslerian surfaces. The Riemannian case will be presented by a collection of basic results (regularity of second root metrics) and formulas up to Gauss curvature. In case of Finslerian surfaces we formulate necessary and sufficient conditions for a locally symmetric fourth root metric in 2D to be positive definite. They are given in terms of the coefficients of the polynomial metric to make checking the positive definiteness as simple and direct as possible. Explicit examples are also presented. The situation is more complicated in case of spaces of dimension more than two. Some necessary conditions and an explicit example are given for a positive definite locally symmetric polynomial metric in 3D. Computations are supported by the MAPLE mathematics software (LinearAlgebra).","Thu, 14 Mar 2024 17:59:09 UTC (370 KB)"
"20","Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models","Ali Nouri, Beatriz Cabrero-Daniel, Fredrik Törner, Hȧkan Sivencrona, Christian Berger","Software Engineering (cs.SE)","DevOps is a necessity in many industries, including the development of Autonomous Vehicles. In those settings, there are iterative activities that reduce the speed of SafetyOps cycles. One of these activities is ""Hazard Analysis & Risk Assessment"" (HARA), which is an essential step to start the safety requirements specification. As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of Large Language Models (LLMs). Our objective is to systematically assess their potential for application in the field of safety engineering. To that end, we propose a framework to support a higher degree of automation of HARA with LLMs. Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly.","Thu, 14 Mar 2024 16:56:52 UTC (261 KB)"
"21","How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions","João Helis Bernardo, Daniel Alencar da Costa, Sérgio Queiroz de Medeiros, Uirá Kulesza","Software Engineering (cs.SE)","Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build durations, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualitative analysis illuminates the discussions around CI in both ML and non-ML projects, encompassing themes like CI Build Execution and Status, CI Testing, and CI Infrastructure. These insights shed light on the unique challenges faced by ML projects in adopting CI practices effectively.","Thu, 14 Mar 2024 16:35:39 UTC (2,328 KB)"
"22","Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub","Francesco Minna, Fabio Massacci, Katja Tuma","Software Engineering (cs.SE)","Background: Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform. A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster. Objective: The goal of this study is to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measure to what extent LLMs could be used for removing misconfiguration. We also want to investigate whether there are false positives in both the LLM refactorings and the tool outputs. Method: We propose a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools, such as Checkov and KICS. First, such a pipeline will run several chart analyzers and identify the common and unique misconfigurations reported by each tool. Secondly, it will use LLMs to suggest mitigation for each misconfiguration. Finally, the chart refactoring previously generated will be analyzed again by the same tools to see whether it satisfies the tool's policies. At the same time, we will also perform a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool's reporting and in the LLM refactoring.","Thu, 14 Mar 2024 16:26:40 UTC (215 KB)"
"23","On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study","Ali Nouri, Christian Berger, Fredrik Törner","Software Engineering (cs.SE)","Safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions. This is especially true for complex AI-enabled and software intensive systems such as Autonomous Drive (AD). System-Theoretic Process Analysis (STPA) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry. However, STPA assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels. This would inhibit software developers from using STPA to analyze their software as part of a bigger system, resulting in a lack of traceability. This can be seen as a maintainability challenge in continuous development and deployment (DevOps). In this paper, STPA's different guidelines for the automotive industry, e.g. J31887/ISO21448/STPA handbook, are firstly compared to assess their applicability to the distributed development of complex AI-enabled systems like AD. Further, an approach to overcome the challenges of using STPA in a multi-level design context is proposed. By conducting an interview study with automotive industry experts for the development of AD, the challenges are validated and the effectiveness of the proposed approach is evaluated.","Thu, 14 Mar 2024 15:56:02 UTC (2,380 KB)"
"24","Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase","Yulong Pei, Salwa Alamir, Rares Dolga, Sameena Shah","Software Engineering (cs.SE)","Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development. This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. However, compared to code defect detection, code revert prediction has been rarely studied in previous research. Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts. Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase. To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import graph with code features. Different strategies to address anomalies and data imbalance have been implemented including graph neural networks with imbalance classification and anomaly detection. We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.","Thu, 14 Mar 2024 15:54:29 UTC (170 KB)"
"25","An Industrial Experience Report about Challenges from Continuous Monitoring, Improvement, and Deployment for Autonomous Driving Features","Ali Nouri, Christian Berger, Fredrik Torner","Software Engineering (cs.SE)","Using continuous development, deployment, and monitoring (CDDM) to understand and improve applications in a customer's context is widely used for non-safety applications such as smartphone apps or web applications to enable rapid and innovative feature improvements. Having demonstrated its potential in such domains, it may have the potential to also improve the software development for automotive functions as some OEMs described on a high level in their financial company communiqus. However, the application of a CDDM strategy also faces challenges from a process adherence and documentation perspective as required by safety-related products such as autonomous driving systems (ADS) and guided by industry standards such as ISO-26262 and ISO21448. There are publications on CDDM in safety-relevant contexts that focus on safety-critical functions on a rather generic level and thus, not specifically ADS or automotive, or that are concentrating only on software and hence, missing out the particular context of an automotive OEM: Well-established legacy processes and the need of their adaptations, and aspects originating from the role of being a system integrator for software/software, hardware/hardware, and hardware/software. In this paper, particular challenges from the automotive domain to better adopt CDDM are identified and discussed to shed light on research gaps to enhance CDDM, especially for the software development of safe ADS. The challenges are identified from today's industrial well-established ways of working by conducting interviews with domain experts and complemented by a literature study.","Thu, 14 Mar 2024 15:14:24 UTC (383 KB)"
"26","Twenty ways to estimate the Log Gaussian Cox Process model with point and aggregated case data: the rts2 package for R","Samuel I Watson","Computation (stat.CO)","The R package rts2 provides data manipulation and model fitting tools for Log Gaussian Cox Process (LGCP) models. LGCP models are a key method for disease and other types of surveillance, and provide a means of predicting risk across an area of interest based on spatially-referenced and time-stamped case data. However, these models can be difficult to specify and computationally demanding to estimate. For many surveillance scenarios we require results in near real-time using routinely available data to guide and direct policy responses, or due to limited availability of computational resources. There are limited software implementations available for this real-time context with reliable predictions and quantification of uncertainty. The rts2 package provides a range of modern Gaussian process approximations and model fitting methods to fit the LGCP, including estimation of covariance parameters, using both Bayesian and stochastic Maximum Likelihood methods. The package provides a suite of data manipulation tools. We also provide a novel implementation to estimate the LGCP when case data are aggregated to an irregular grid such as census tract areas.","Thu, 14 Mar 2024 14:46:59 UTC (2,983 KB)"
"27","LLM-based agents for automating the enhancement of user story quality: An early report","Zheying Zhang, Maruf Rayhan, Tomas Herda, Manuel Goisauf, Pekka Abrahamsson","Software Engineering (cs.SE)","In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous LLM-based Agent System and implemented it at the company. The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams. Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting.","Thu, 14 Mar 2024 14:35:53 UTC (1,105 KB)"
"28","Binomial sums and Mellin asymptotics with explicit error bounds: a case study","Benjamin Hackl, Stephan Wagner","Combinatorics (math.CO)","Making use of a newly developed package in the computer algebra system SageMath, we show how to perform a full asymptotic analysis by means of the Mellin transform with explicit error bounds. As an application of the method, we answer a question of Bóna and DeJonge on 132-avoiding permutations with a unique longest increasing subsequence that can be translated into an inequality for a certain binomial sum.","Thu, 14 Mar 2024 14:01:19 UTC (29 KB)"
"29","An Extensible Framework for Architecture-Based Data Flow Analysis for Information Security","Nicolas Boltz, Sebastian Hahner, Christopher Gerking, Robert Heinrich","Software Engineering (cs.SE)","The growing interconnection between software systems increases the need for security already at design time. Security-related properties like confidentiality are often analyzed based on data flow diagrams (DFDs). However, manually analyzing DFDs of large software systems is bothersome and error-prone, and adjusting an already deployed software is costly. Additionally, closed analysis ecosystems limit the reuse of modeled information and impede comprehensive statements about a system's security. In this paper, we present an open and extensible framework for data flow analysis. The central element of our framework is our new implementation of a well-validated data-flow-based analysis approach. The framework is compatible with DFDs and can also extract data flows from the Palladio architectural description language. We showcase the extensibility with multiple model and analysis extensions. Our evaluation indicates that we can analyze similar scenarios while achieving higher scalability compared to previous implementations.","Thu, 14 Mar 2024 13:52:41 UTC (404 KB)"
"30","HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation","Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Ying Shan, Xiaohang Zhan, Zeyu Wang","Graphics (cs.GR)","We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seamlessly in 3D graphics software, facilitating downstream applications such as more efficient animation with inherited blend shapes and semantic consistency.","Thu, 14 Mar 2024 12:15:23 UTC (32,531 KB)"
"31","PWACG: Partial Wave Analysis Code Generator supporting Newton-conjugate gradient method","Xiang Dong, Yu-Chang Sun, Chu-Cheng Pan, Ao-Yan Cheng, Ao-Bo Wang, Hao Cai, Kai Zhu","Computational Physics (physics.comp-ph)","This paper introduces a novel Partial Wave Analysis Code Generator (PWACG) that automatically generates high-performance partial wave analysis codes. This is achieved by leveraging the JAX automatic differentiation library and the jinja2 template engine. The resulting code is constructed using the high-performance API of JAX, and includes support for the Newton's Conjugate Gradient optimization method, as well as the full utilization of parallel computing capabilities offered by GPUs. By harnessing these advanced computing techniques, PWACG demonstrates a significant advantage in efficiently identifying global optimal points compared to conventional partial wave analysis software packages.","Thu, 14 Mar 2024 09:43:24 UTC (751 KB)"
"32","An Extensive Comparison of Static Application Security Testing Tools","Matteo Esposito, Valentina Falaschi, Davide Falessi","Software Engineering (cs.SE)","Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications. Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision. Aim: We aim to comprehensively evaluate SASTTs, setting a reliable benchmark for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives. Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase. It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis. Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities. In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall. Conclusions: The paper suggests that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions.","Thu, 14 Mar 2024 09:37:54 UTC (1,235 KB)"
"33","CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences","Martin Weyssow, Aton Kamanda, Houari Sahraoui","Software Engineering (cs.SE)","Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that CodeLlama-7B-Instruct, aligned through reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference tuning. Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model. Therefore, our contributions bridge the gap in preference tuning of LLMs for code and set the stage for further advancements in model alignment and RLAIF for code intelligence. Our code and data are available at this https URL.","Thu, 14 Mar 2024 01:51:35 UTC (999 KB)"
"34","FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices","Arnab Raha, Deepak A. Mathaikutty, Soumendu K. Ghosh, Shamik Kundu","Hardware Architecture (cs.AR)","This paper introduces FlexNN, a Flexible Neural Network accelerator, which adopts agile design principles to enable versatile dataflows, enhancing energy efficiency. Unlike conventional convolutional neural network accelerator architectures that adhere to fixed dataflows (such as input, weight, output, or row stationary) for transferring activations and weights between storage and compute units, our design revolutionizes by enabling adaptable dataflows of any type through software configurable descriptors. Considering that data movement costs considerably outweigh compute costs from an energy perspective, the flexibility in dataflow allows us to optimize the movement per layer for minimal data transfer and energy consumption, a capability unattainable in fixed dataflow architectures. To further enhance throughput and reduce energy consumption in the FlexNN architecture, we propose a novel sparsity-based acceleration logic that utilizes fine-grained sparsity in both the activation and weight tensors to bypass redundant computations, thus optimizing the convolution engine within the hardware accelerator. Extensive experimental results underscore a significant enhancement in the performance and energy efficiency of FlexNN relative to existing DNN accelerators.","Thu, 14 Mar 2024 01:39:12 UTC (13,526 KB)"
"35","Quantum Annealing Approach for the Optimal Real-time Traffic Control using QUBO","Amit Singh, Chun-Yu Lin, Chung-I Huang, Fang-Pang Lin","Quantum Physics (quant-ph)","Traffic congestion is one of the major issues in urban areas, particularly when traffic loads exceed the roads capacity, resulting in higher petrol consumption and carbon emissions as well as delays and stress for road users. In Asia, the traffic situation can be further deteriorated by road sharing of scooters. How to control the traffic flow to mitigate the congestion has been one of the central issues in transportation research. In this study, we employ a quantum annealing approach to optimize the traffic signals control at a real-life intersection with mixed traffic flows of vehicles and scooters. Considering traffic flow is a continuous and emerging phenomenon, we used quadratic unconstrained binary optimization (QUBO) formalism for traffic optimization, which has a natural equivalence to the Ising model and can be solved efficiently on the quantum annealers, quantum computers or digital annealers. In this article, we first applied the QUBO traffic optimization to artificially generated traffic for a simple intersection, and then we used real-time traffic data to simulate a real Dongda-Keyuan intersection with dedicated cars and scooter lanes, as well as mixed scooter and car lanes. We introduced two types of traffic light control systems for traffic optimization C-QUBO and QUBO. Our rigorous QUBO optimizations show that C-QUBO and QUBO outperform the commonly used fixed cycle method, with QUBO outperforming C-QUBO in some instances. It has been found that QUBO optimization significantly relieves traffic congestion for the unbalanced traffic volume. Furthermore, we found that dynamic changes in traffic light signal duration greatly reduce traffic congestion.","Thu, 14 Mar 2024 01:24:19 UTC (1,909 KB)"
"36","Leveraging the Crowd for Dependency Management: An Empirical Study on the Dependabot Compatibility Score","Benjamin Rombaut, Filipe R. Cogo, Ahmed E. Hassan","Software Engineering (cs.SE)","Dependabot, a popular dependency management tool, includes a compatibility score feature that helps client packages assess the risk of accepting a dependency update by leveraging knowledge from ""the crowd"". For each dependency update, Dependabot calculates this compatibility score as the proportion of successful updates performed by other client packages that use the same provider package as a dependency. In this paper, we study the efficacy of the compatibility score to help client packages assess the risks involved with accepting a dependency update. We analyze 579,206 pull requests opened by Dependabot to update a dependency, along with 618,045 compatibility score records calculated by Dependabot. We find that a compatibility score cannot be calculated for 83% of the dependency updates due to the lack of data from the crowd. Yet, the vast majority of the scores that can be calculated have a small confidence interval and are based on low-quality data, suggesting that client packages should have additional angles to evaluate the risk of an update and the trustworthiness of the compatibility score. To overcome these limitations, we propose metrics that amplify the input from the crowd and demonstrate the ability of those metrics to predict the acceptance of a successful update by client packages. We also demonstrate that historical update metrics from client packages can be used to provide a more personalized compatibility score. Based on our findings, we argue that, when leveraging the crowd, dependency management bots should include a confidence interval to help calibrate the trust clients can place in the compatibility score, and consider the quality of tests that exercise candidate updates.","Thu, 14 Mar 2024 00:26:19 UTC (4,936 KB)"
"37","Bugs in Large Language Models Generated Code","Florian Tambon, Arghavan Moradi Dakhel, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais, Giuliano Antoniol","Software Engineering (cs.SE)","Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.","Wed, 13 Mar 2024 20:12:01 UTC (1,323 KB)"
"38","Loop unrolling (for test coverage): formal definition","Bertrand Meyer","Software Engineering (cs.SE)","Techniques to achieve various forms of test coverage, such as branch coverage, typically do not iterate loops; in other words, they treat a loop as a conditional, executed zero or one time. Existing work by the author and collaborators produces test suites guaranteeing full branch coverage. More recent work has shown that by unrolling loops the approach can find significantly more bugs. The present discussion provides the theoretical basis and precise definition for this concept of unrolling.","Wed, 13 Mar 2024 19:28:04 UTC (271 KB)"
"39","The terascale tutorial","Konstantinos Theofilatos","High Energy Physics - Experiment (hep-ex)","This note summarizes the lectures given in the tutorial session of the Introduction to the Terascale school at DESY on March 2023. The target audience are advanced bachelor and master physics students. The tutorial aims to best prepare the students for starting an LHC experimental physics thesis. The cross section of the top quark pair production is detailed alongside with the reconstruction of the invariant masses of the top quark as well as of the $W$ and $Z$ bosons. The tutorial uses ideas and CMS open data files from the CMS HEP Tutorial written by C. Sander and A. Schmidt, but is entirely rewritten so that it can be run in Google Colab Cloud in a columnar style of analysis with python. In addition, a minimal C/C++ version of a simple event-loop analysis relying on ROOT is exampled. The code is kept as short as possible with emphasis on the transparency of the analysis steps, rather than the elegance of the software, having in mind that the students will in any case need to rewrite their own custom analysis framework.","Wed, 13 Mar 2024 18:56:35 UTC (562 KB)"
"40","Measuring the bioeconomy economically: exploring the connections between concepts, methods, data, indicators and their limitations","Sebastián Leavy, Gabriela Allegretti, Elen Presotto, Marco Antonio Montoya, Edson Talamini","General Economics (econ.GN)","Despite its relevance, measuring the contributions of the bioeconomy to national economies remains an arduous task that faces limitations. Part of the difficulty is associated with the lack of a clear and widely accepted concept of the bioeconomy and moves on to the connections between methods, data and indicators. The present study aims to define the concepts of bioeconomy and to explore the connections between concepts, methods, data and indicators when measuring the bioeconomy economically, and the limitations involved in this process. The bioeconomy concepts were defined based on a literature review and a content analysis of 84 documents selected through snowballing procedures to find articles measuring 'how big is the bioeconomy?'. The content of the 84 documents was uploaded to the QDA Miner software and coded according to the bioeconomy concept, the methods or models used, the data sources accessed, the indicators calculated, and the limitations reported by the authors. The results of the occurrence and co-occurrence of the codes were extracted and analyzed statistically, indicating that the measurement of bioeconomy (i) need recognize and pursue the proposed concept of holistic bioeconomy; (ii) rarely considered aspects of holistic bioeconomy (3.5%); (iii) is primarily based on the concept of biomass-based bioeconomy (BmBB) (94%); (iv) the association with the concept of biosphere (BsBB) appeared in 26% of the studies; (v) the biotech-based bioeconomy (BtBB) was the least frequent (1.2%); (vi) there is a diversity of methods and models, but the most common are those traditionally used to measure macroeconomic activities, especially input-output models; (vii) depending on the prevailing methods, the data comes from various official statistical databases, such as national accounts and economic activity classification systems;...","Wed, 13 Mar 2024 18:14:32 UTC (10,634 KB)"
"41","Cultural evolution in populations of Large Language Models","Jérémy Perez, Corentin Léger, Marcela Ovando-Tellez, Chris Foulon, Joan Dussauld, Pierre-Yves Oudeyer, Clément Moulin-Frier","Multiagent Systems (cs.MA)","Research in cultural evolution aims at providing causal explanations for the change of culture over time. Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods. While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models. This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms. We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap. On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake. Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution. We here present a framework for simulating cultural evolution in populations of LLMs, allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed. The software we developed for conducting these simulations is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence.","Wed, 13 Mar 2024 18:11:17 UTC (11,190 KB)"
"42","Automating SBOM Generation with Zero-Shot Semantic Similarity","Devin Pereira, Christopher Molloy, Sudipta Acharya, Steven H.H. Ding","Software Engineering (cs.SE)","It is becoming increasingly important in the software industry, especially with the growing complexity of software ecosystems and the emphasis on security and compliance for manufacturers to inventory software used on their systems. A Software-Bill-of-Materials (SBOM) is a comprehensive inventory detailing a software application's components and dependencies. Current approaches rely on case-based reasoning to inconsistently identify the software components embedded in binary files. We propose a different route, an automated method for generating SBOMs to prevent disastrous supply-chain attacks. Remaining on the topic of static code analysis, we interpret this problem as a semantic similarity task wherein a transformer model can be trained to relate a product name to corresponding version strings. Our test results are compelling, demonstrating the model's strong performance in the zero-shot classification task, further demonstrating the potential for use in a real-world cybersecurity context.","Sat, 3 Feb 2024 18:14:13 UTC (391 KB)"
"43","Self-adaptive, Requirements-driven Autoscaling of Microservices","João Paulo Karol Santos Nunes, Shiva Nejati, Mehrdad Sabetzadeh, Elisa Yumi Nakagawa","Distributed, Parallel, and Cluster Computing (cs.DC)","Microservices architecture offers various benefits, including granularity, flexibility, and scalability. A crucial feature of this architecture is the ability to autoscale microservices, i.e., adjust the number of replicas and/or manage resources. Several autoscaling solutions already exist. Nonetheless, when employed for diverse microservices compositions, current solutions may exhibit suboptimal resource allocations, either exceeding the actual requirements or falling short. This can in turn lead to unbalanced environments, downtime, and undesirable infrastructure costs. We propose MS-RA, a self-adaptive, requirements-driven solution for microservices autoscaling. MS-RA utilizes service-level objectives (SLOs) for real-time decision making. Our solution, which is customizable to specific needs and costs, facilitates a more efficient allocation of resources by precisely using the right amount to meet the defined requirements. We have developed MS-RA based on the MAPE-K self-adaptive loop, and have evaluated it using an open-source microservice-based application. Our results indicate that MS-RA considerably outperforms the horizontal pod autoscaler (HPA), the industry-standard Kubernetes autoscaling mechanism. It achieves this by using fewer resources while still ensuring the satisfaction of the SLOs of interest. Specifically, MS-RA meets the SLO requirements of our case-study system, requiring at least 50% less CPU time, 87% less memory, and 90% fewer replicas compared to the HPA.","Fri, 2 Feb 2024 03:00:05 UTC (2,858 KB)"
"44","Constrained Reinforcement Learning for Adaptive Controller Synchronization in Distributed SDN","Ioannis Panitsas, Akrit Mudvari, Leandros Tassiulas","Networking and Internet Architecture (cs.NI)","In software-defined networking (SDN), the implementation of distributed SDN controllers, with each controller responsible for managing a specific sub-network or domain, plays a critical role in achieving a balance between centralized control, scalability, reliability, and network efficiency. These controllers must be synchronized to maintain a logically centralized view of the entire network. While there are various approaches for synchronizing distributed SDN controllers, most tend to prioritize goals such as optimization of communication latency or load balancing, often neglecting to address both the aspects simultaneously. This limitation becomes particularly significant when considering applications like Augmented and Virtual Reality (AR/VR), which demand constrained network latencies and substantial computational resources. Additionally, many existing studies in this field predominantly rely on value-based reinforcement learning (RL) methods, overlooking the potential advantages offered by state-of-the-art policy-based RL algorithms. To bridge this gap, our work focuses on examining deep reinforcement learning (DRL) techniques, encompassing both value-based and policy-based methods, to guarantee an upper latency threshold for AR/VR task offloading within SDN environments, while selecting the most cost-effective servers for AR/VR task offloading. Our evaluation results indicate that while value-based methods excel in optimizing individual network metrics such as latency or load balancing, policy-based approaches exhibit greater robustness in adapting to sudden network changes or reconfiguration.","Sun, 21 Jan 2024 21:57:22 UTC (1,687 KB)"
"45","QCSHQD: Quantum computing as a service for Hybrid classical-quantum software development: A Vision","Arif Ali Khan, Maryam Tavassoli Sabzevari, Davide Taibi, Matteo Esposito","Software Engineering (cs.SE)","Quantum Computing (QC) is transitioning from theoretical frameworks to an indispensable powerhouse of computational capability, resulting in extensive adoption across both industrial and academic domains. QC presents exceptional advantages, including unparalleled processing speed and the potential to solve complex problems beyond the capabilities of classical computers. Nevertheless, academic researchers and industry practitioners encounter various challenges in harnessing the benefits of this technology. The limited accessibility of QC resources for classical developers, and a general lack of domain knowledge and expertise, represent insurmountable barrier, hence to address these challenges, we introduce a framework- Quantum Computing as a Service for Hybrid Classical-Quantum Software Development (QCSHQD), which leverages service-oriented strategies. Our framework comprises three principal components: an Integrated Development Environment (IDE) for user interaction, an abstraction layer dedicated to orchestrating quantum services, and a service provider responsible for executing services on quantum computer. This study presents a blueprint for QCSHQD, designed to democratize access to QC resources for classical developers who want to seamless harness QC power. The vision of QCSHQD paves the way for groundbreaking innovations by addressing key challenges of hybridization between classical and quantum computers.","Wed, 13 Mar 2024 16:16:43 UTC (683 KB)"
"46","Predicting long timescale kinetics under variable experimental conditions with Kinetica.jl","Joe Gilkes (1,2), Mark Storr (3), Reinhard J. Maurer (1,4), Scott Habershon (1) ((1) Department of Chemistry, University of Warwick, (2) EPSRC HetSys Centre for Doctoral Training, University of Warwick, (3) AWE Plc, (4) Department of Physics, University of Warwick)","Chemical Physics (physics.chem-ph)","Predicting the degradation processes of molecules over long timescales is a key aspect of industrial materials design. However, it is made computationally challenging by the need to construct large networks of chemical reactions that are relevant to the experimental conditions that kinetic models must mirror, with every reaction requiring accurate kinetic data. Here we showcase Kinetica.jl, a new software package for constructing large-scale chemical reaction networks in a fully-automated fashion by exploring chemical reaction space with a kinetics-driven algorithm; coupled to efficient machine-learning models of activation energies for sampled elementary reactions, we show how this approach readily enables generation and kinetic characterization of networks containing $\sim10^{3}$ chemical species and $10^{4}$ - $10^{5}$ reactions. Symbolic-numeric modelling of the generated reaction networks is used to allow for flexible, efficient computation of kinetic profiles under experimentally-realizable conditions such as continuously-variable temperature regimes, enabling direct connection between bottom-up reaction networks and experimental observations. Highly efficient propagation of long-timescale kinetic profiles is required for automated reaction network refinement and is enabled here by a new discrete kinetic approximation. The resulting Kinetica.jl simulation package therefore enables automated generation, characterization, and long-timescale modelling of complex chemical reaction systems. We demonstrate this for hydrocarbon pyrolysis simulated over timescales of seconds, using transient temperature profiles representing those of tubular flow reactor experiments.","Wed, 13 Mar 2024 16:10:11 UTC (5,438 KB)"
"47","Physical Memory Attacks and a Memory Safe Management System for Memory Defense","Alon Hillel-Tuch, Aspen Olmstead","Cryptography and Security (cs.CR)","Programming errors, defective hardware components (such as hard disk spindle defects), and environmental hazards can lead to invalid memory operations. In addition, less predictable forms of environmental stress, such as radiation, thermal influence, and energy fluctuations, can induce hardware faults. Sometimes, a soft error can occur instead of a complete failure, such as a bit-flip. The 'natural' factors that can cause bit-flips are replicable through targeted attacks that result in significant compromises, including full privileged system access. Existing physical defense solutions have consistently been circumvented shortly after deployment. We will explore the concept of a novel software-based low-level layer that can protect vulnerable memory targeted by physical attack vectors related to bit-flip vulnerabilities.","Wed, 13 Mar 2024 16:10:04 UTC (406 KB)"
"48","TopoTB: A software package for calculating the electronic structure and topological properties of the tight-binding model","Xinliang Huang, Fawei Zheng, Ning Hao","Materials Science (cond-mat.mtrl-sci)","We present TopoTB, a software package written in the Mathematica language, designed to compute electronic structures, topological properties, and phase diagrams based on tight-binding models. TopoTB is user-friendly, with an interactive user interface that enables the tuning of model parameters for fitting the target energy bands in a WYSIWYG way. In addition, TopoTB also includes functionalities for processing results from Density Functional Theory calculations. The outputs of TopoTB are rich and readable, and they can be displayed in various styles. These features make TopoTB a useful tool for the theoretical study of materials.","Wed, 13 Mar 2024 15:27:05 UTC (13,923 KB)"
"49","DevBench: A Comprehensive Benchmark for Software Development","Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua Lin, Chao Peng, Kai Chen","Computation and Language (cs.CL)","Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications. Our benchmark is available at this https URL","Wed, 13 Mar 2024 15:13:44 UTC (10,471 KB)[v2] Fri, 15 Mar 2024 13:23:34 UTC (10,471 KB)"
"50","CAM: A Collection of Snapshots of GitHub Java Repositories Together with Metrics","Yegor Bugayenko","Software Engineering (cs.SE)","Even though numerous researchers require stable datasets along with source code and basic metrics calculated on them, neither GitHub nor any other code hosting platform provides such a resource. Consequently, each researcher must download their own data, compute the necessary metrics, and then publish the dataset somewhere to ensure it remains accessible indefinitely. Our CAM (stands for ``Classes and Metrics'') project addresses this need. It is an open-source software capable of cloning Java repositories from GitHub, filtering out unnecessary files, parsing Java classes, and computing metrics such as Cyclomatic Complexity, Halstead Effort and Volume, C\&K metrics, Maintainability Metrics, LCOM5 and HND, as well as some Git-based Metrics. At least once a year, we execute the entire script, a process which requires a minimum of ten days on a very powerful server, to generate a new dataset. Subsequently, we publish it on Amazon S3, thereby ensuring its availability as a reference for researchers. The latest archive of 2.2Gb that we published on the 2nd of March, 2024 includes 532K Java classes with 48 metrics for each class.","Wed, 13 Mar 2024 12:52:57 UTC (10 KB)"
"51","Understanding and Evaluating Developer Behaviour in Programming Tasks","Martin Schröer, Rainer Koschke","Software Engineering (cs.SE)","To evaluate how developers perform differently in solving programming tasks, i.e., which actions and behaviours are more beneficial to them than others and if there are any specific strategies and behaviours that may indicate good versus poor understanding of the task and program given to them, we used the MIMESIS plug-in to record developers' interactions with the IDE. In a series of three studies we investigated the specific behaviour of developers solving a specific programming task. We focused on which source code files they visited, how they related pieces of code and knowledge to others and when and how successful they performed code edits. To cope with the variety of behaviours due to interpersonal differences such as different level of knowledge, development style or problem solving stratiegies, we used an abstraction of the observed behaviour, which enables for a better comparison between different individual attributes such as skill, speed and used stratiegies and also facilitates later automatic evaluation of behaviours, i.e. by using a software to react to.","Wed, 13 Mar 2024 12:46:42 UTC (161 KB)"
"52","An Integrated Usability Framework for Evaluating Open Government Data Portals: Comparative Analysis of EU and GCC Countries","Fillip Molodtsov, Anastasija Nikiforova","Computers and Society (cs.CY)","This study explores the critical role of open government data (OGD) portals in fostering transparency and collaboration between diverse stakeholders. Recognizing the challenges of usability, communication with diverse populations, and strategic value creation, this paper develops an integrated framework for evaluating OGD portal effectiveness that accommodates user diversity (regardless of their data literacy and language), evaluates collaboration and participation, and the ability of users to explore and understand the data provided through them. The framework is validated by applying it to 33 national portals across European Union and Gulf Cooperation Council (GCC) countries, as a result of which we rank OGD portals, identify some good practices that lower-performing portals can learn from, and common shortcomings. Notably, the study unveils the competitive and innovative nature of GCC OGD portals, pinpointing specific improvement areas such as multilingual support and data understandability. The findings underscore the growing trend of exposing data quality metrics and advocate for enhanced two-way communication channels between users and portal representatives. Overall, the study contributes to accelerating the development of user-friendly, collaborative, and sustainable OGD portals while addressing gaps identified in previous research.","Wed, 13 Mar 2024 12:06:42 UTC (745 KB)"
"53","Search-based Optimisation of LLM Learning Shots for Story Point Estimation","Vali Tawosi, Salwa Alamir, Xiaomo Liu","Software Engineering (cs.SE)","One of the ways Large Language Models (LLMs) are used to perform machine learning tasks is to provide them with a few examples before asking them to produce a prediction. This is a meta-learning process known as few-shot learning. In this paper, we use available Search-Based methods to optimise the number and combination of examples that can improve an LLM's estimation performance, when it is used to estimate story points for new agile tasks. Our preliminary results show that our SBSE technique improves the estimation performance of the LLM by 59.34% on average (in terms of mean absolute error of the estimation) over three datasets against a zero-shot setting.","Wed, 13 Mar 2024 11:29:37 UTC (44 KB)"
"54","Software Vulnerability and Functionality Assessment using LLMs","Rasmus Ingemann Tuffveson Jensen, Vali Tawosi, Salwa Alamir","Software Engineering (cs.SE)","While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities.","Wed, 13 Mar 2024 11:29:13 UTC (26 KB)"
"55","System for systematic literature review using multiple AI agents: Concept and an empirical evaluation","Abdul Malik Sami, Zeeshan Rasheed, Kai-Kristian Kemell, Muhammad Waseem, Terhi Kilamo, Mika Saari, Anh Nguyen Duc, Kari Systä, Pekka Abrahamsson","Software Engineering (cs.SE)","Systematic Literature Reviews (SLRs) have become the foundation of evidence-based studies, enabling researchers to identify, classify, and combine existing studies based on specific research questions. Conducting an SLR is largely a manual process. Over the previous years, researchers have made significant progress in automating certain phases of the SLR process, aiming to reduce the effort and time needed to carry out high-quality SLRs. However, there is still a lack of AI agent-based models that automate the entire SLR process. To this end, we introduce a novel multi-AI agent model designed to fully automate the process of conducting an SLR. By utilizing the capabilities of Large Language Models (LLMs), our proposed model streamlines the review process, enhancing efficiency and accuracy. The model operates through a user-friendly interface where researchers input their topic, and in response, the model generates a search string used to retrieve relevant academic papers. Subsequently, an inclusive and exclusive filtering process is applied, focusing on titles relevant to the specific research area. The model then autonomously summarizes the abstracts of these papers, retaining only those directly related to the field of study. In the final phase, the model conducts a thorough analysis of the selected papers concerning predefined research questions. We also evaluated the proposed model by sharing it with ten competent software engineering researchers for testing and analysis. The researchers expressed strong satisfaction with the proposed model and provided feedback for further improvement. The code for this project can be found on the GitHub repository at this https URL.","Wed, 13 Mar 2024 10:27:52 UTC (2,032 KB)"
"56","A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance","Bruno Pereira Cipriano, Pedro Alves, Paul Denny","Software Engineering (cs.SE)","Much research has highlighted the impressive capabilities of large language models (LLMs), like GPT and Bard, for solving introductory programming exercises. Recent work has shown that LLMs can effectively solve a range of more complex object-oriented programming (OOP) exercises with text-based specifications. This raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking. To address this, we propose an innovative approach to formulating OOP tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in OOP courses. We introduce a novel notation system for specifying OOP assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester. Student perceptions of this approach are explored through a survey (n=56). Generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises. This notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects. Furthermore, students reported being less inclined to rely on LLM-based code generation tools for these diagram and video-based exercises. Experiments with GPT-4 and Bard's vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate code solutions.","Wed, 13 Mar 2024 10:21:29 UTC (1,026 KB)"
"57","Translating between SQL Dialects for Cloud Migration","Ran Zmigrod, Salwa Alamir, Xiaomo Liu","Databases (cs.DB)","Migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions. A crucial component of such cloud migrations is the transition of databases to be hosted online. In this work, we consider the difficulties of this migration for SQL databases. While SQL is one of the prominent methods for storing database procedures, there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.) which can complicate migrations when the on-premise SQL dialect differs to the dialect hosted on the cloud. Tools exist by common cloud provides such as AWS and Azure to aid in translating between dialects in order to mitigate the majority of the difficulties. However, these tools do not successfully translate $100\%$ of the code. Consequently, software engineers must manually convert the remainder of the untranslated database. For large organizations, this task quickly becomes intractable and so more innovative solutions are required. We consider this challenge a novel yet vital industrial research problem for any large corporation that is considering cloud migrations. Furthermore, we introduce potential avenues of research to tackle this challenge that have yielded promising preliminary results.","Wed, 13 Mar 2024 09:38:39 UTC (64 KB)"
"58","Log Summarisation for Defect Evolution Analysis","Rares Dolga, Ran Zmigrod, Rui Silva, Salwa Alamir, Sameena Shah","Software Engineering (cs.SE)","Log analysis and monitoring are essential aspects in software maintenance and identifying defects. In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time? While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods. In this work, we suggest an online semantic-based clustering approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles. We also introduce a novel metric to evaluate the performance of temporal log clusters. We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems. We hope that our work encourages further temporal exploration in defect datasets.","Wed, 13 Mar 2024 09:18:46 UTC (187 KB)"
"59","DONAPI: Malicious NPM Packages Detector using Behavior Sequence Knowledge Mapping","Cheng Huang (1), Nannan Wang (1), Ziyan Wang (1), Siqi Sun (1), Lingzi Li (1), Junren Chen (1), Qianchong Zhao (1), Jiaxuan Han (1), Zhen Yang (1), Lei Shi (2) ((1) Sichuan University, (2) Huawei Technologies)","Cryptography and Security (cs.CR)","With the growing popularity of modularity in software development comes the rise of package managers and language ecosystems. Among them, npm stands out as the most extensive package manager, hosting more than 2 million third-party open-source packages that greatly simplify the process of building code. However, this openness also brings security risks, as evidenced by numerous package poisoning incidents. In this paper, we synchronize a local package cache containing more than 3.4 million packages in near real-time to give us access to more package code details. Further, we perform manual inspection and API call sequence analysis on packages collected from public datasets and security reports to build a hierarchical classification framework and behavioral knowledge base covering different sensitive behaviors. In addition, we propose the DONAPI, an automatic malicious npm packages detector that combines static and dynamic analysis. It makes preliminary judgments on the degree of maliciousness of packages by code reconstruction techniques and static analysis, extracts dynamic API call sequences to confirm and identify obfuscated content that static analysis can not handle alone, and finally tags malicious software packages based on the constructed behavior knowledge base. To date, we have identified and manually confirmed 325 malicious samples and discovered 2 unusual API calls and 246 API call sequences that have not appeared in known samples.","Wed, 13 Mar 2024 08:38:21 UTC (943 KB)"
"60","From Channel Measurement to Training Data for PHY Layer AI Applications","Michael Zentarra, Julian Ahrens, Lia Ahrens","Networking and Internet Architecture (cs.NI)","Learning-based techniques such as artificial intelligence (AI) and machine learning (ML) play an increasingly important role in the development of future communication networks. The success of a learning algorithm depends on the quality and quantity of the available training data. In the physical layer (PHY), channel information data can be obtained either through measurement campaigns or through simulations based on predefined channel models. Performing measurements can be time consuming while only gaining information about one specific position or scenario. Simulated data, on the other hand, are more generalized and reflect in most cases not a real environment but instead, a statistical approximation based on a mathematical model. This paper presents a procedure for acquiring channel data by means of fast and flexible software defined radio (SDR) based channel measurements along with a method for a parameter extraction that provides configuration input to the simulator. The procedure from the measurement to the simulated channel data is demonstrated in two exemplary propagation scenarios. It is shown, that in both cases the simulated data is in good accordance to the measurements","Wed, 13 Mar 2024 07:54:01 UTC (4,934 KB)"
"61","When Code Smells Meet ML: On the Lifecycle of ML-specific Code Smells in ML-enabled Systems","Gilberto Recupito, Giammaria Giordano, Filomena Ferrucci, Dario Di Nucci, Fabio Palomba","Software Engineering (cs.SE)","Context. The adoption of Machine Learning (ML)--enabled systems is steadily increasing. Nevertheless, there is a shortage of ML-specific quality assurance approaches, possibly because of the limited knowledge of how quality-related concerns emerge and evolve in ML-enabled systems. Objective. We aim to investigate the emergence and evolution of specific types of quality-related concerns known as ML-specific code smells, i.e., sub-optimal implementation solutions applied on ML pipelines that may significantly decrease both the quality and maintainability of ML-enabled systems. More specifically, we present a plan to study ML-specific code smells by empirically analyzing (i) their prevalence in real ML-enabled systems, (ii) how they are introduced and removed, and (iii) their survivability. Method. We will conduct an exploratory study, mining a large dataset of ML-enabled systems and analyzing over 400k commits about 337 projects. We will track and inspect the introduction and evolution of ML smells through CodeSmile, a novel ML smell detector that we will build to enable our investigation and to detect ML-specific code smells.","Wed, 13 Mar 2024 07:43:45 UTC (1,355 KB)"
"62","AutoDev: Automated AI-Driven Development","Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, Neel Sundaresan","Software Engineering (cs.SE)","The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They also have access to files, compiler output, build and testing logs, static analysis tools, and more. This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers. This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev. In our evaluation, we tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and 87.8% of Pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment.","Wed, 13 Mar 2024 07:12:03 UTC (273 KB)"
"63","Versatile Defense Against Adversarial Attacks on Image Recognition","Haibo Zhang, Zhihua Yao, Kouichi Sakurai","Computer Vision and Pattern Recognition (cs.CV)","Adversarial attacks present a significant security risk to image recognition tasks. Defending against these attacks in a real-life setting can be compared to the way antivirus software works, with a key consideration being how well the defense can adapt to new and evolving attacks. Another important factor is the resources involved in terms of time and cost for training defense models and updating the model database. Training many models that are specific to each type of attack can be time-consuming and expensive. Ideally, we should be able to train one single model that can handle a wide range of attacks. It appears that a defense method based on image-to-image translation may be capable of this. The proposed versatile defense approach in this paper only requires training one model to effectively resist various unknown adversarial attacks. The trained model has successfully improved the classification accuracy from nearly zero to an average of 86%, performing better than other defense methods proposed in prior studies. When facing the PGD attack and the MI-FGSM attack, versatile defense model even outperforms the attack-specific models trained based on these two attacks. The robustness check also shows that our versatile defense model performs stably regardless with the attack strength.","Wed, 13 Mar 2024 01:48:01 UTC (5,509 KB)"
"64","RoboCertProb: Property Specification for Probabilistic RoboChart Models","Kangfeng Ye, Jim Woodcock","Logic in Computer Science (cs.LO)","RoboChart is a core notation in the RoboStar framework which brings modern modelling and formal verification technologies into software engineering for robotics. It is a timed and probabilistic domain-specific language for robotics and provides a UML-like architectural and state machine modelling. This work presents RoboCertProb for specifying quantitative properties of probabilistic robotic systems modelled in RoboChart. RoboCertProb's semantics is based on PCTL*. To interpret RoboCertProb over RoboChart models, we give a Markov semantics (DTMCs and MDPs) to RoboChart, derived from its existing transformation semantics to the PRISM language. In addition to property specification, RoboCertProb also entitles us to configure loose constants and unspecified functions and operations in RoboChart models. It allows us to set up environmental inputs to verify reactive probabilistic systems not directly supported in probabilistic model checkers like PRISM because they employ a closed-world assumption. We implement RoboCertProb in an accompanying tool of RoboChart, RoboTool, for specifying properties and automatically generating PRISM properties from them to formally verify RoboChart models using PRISM. We have used it to analyse the behaviour of software controllers for two real robots: an industrial painting robot and an agricultural robot for treating plants with UV lights.","Tue, 12 Mar 2024 23:47:00 UTC (1,188 KB)"
"65","Assessing the Influence of Toxic and Gender Discriminatory Communication on Perceptible Diversity in OSS Projects","Sayma Sultana, Gias Uddin, Amiangshu Bosu","Software Engineering (cs.SE)","The presence of toxic and gender-identity derogatory language in open-source software (OSS) communities has recently become a focal point for researchers. Such comments not only lead to frustration and disengagement among developers but may also influence their leave from the OSS projects. Despite ample evidence suggesting that diverse teams enhance productivity, the existence of toxic or gender identity discriminatory communications poses a significant threat to the participation of individuals from marginalized groups and, as such, may act as a barrier to fostering diversity and inclusion in OSS projects. However, there is a notable lack of research dedicated to exploring the association between gender-based toxic and derogatory language with a perceptible diversity of open-source software teams. Consequently, this study aims to investigate how such content influences the gender, ethnicity, and tenure diversity of open-source software development teams. To achieve this, we extract data from active GitHub projects, assess various project characteristics, and identify instances of toxic and gender-discriminatory language within issue/pull request comments. Using these attributes, we construct a regression model to explore how they associate with the perceptible diversity of those projects.","Tue, 12 Mar 2024 22:48:21 UTC (152 KB)[v2] Thu, 14 Mar 2024 22:07:48 UTC (147 KB)"
"66","EXCOGITO, an extensible coarse-graining toolbox for the investigation of biomolecules by means of low-resolution representation","Marco Giulini, Raffaele Fiorentini, Luca Tubiana, Raffaello Potestio, Roberto Menichetti","Soft Condensed Matter (cond-mat.soft)","Bottom-up coarse-grained (CG) models proved to be essential to complement and sometimes even replace all-atom representations of soft matter systems and biological macromolecules. The development of low-resolution models takes the moves from the reduction of the degrees of freedom employed, that is, the definition of a mapping between a system's high-resolution description and its simplified counterpart. Even in absence of an explicit parametrisation and simulation of a CG model, the observation of the atomistic system in simpler terms can be informative: this idea is leveraged by the mapping entropy, a measure of the information loss inherent to the process of coarsening. Mapping entropy lies at the heart of the extensible coarse-graining toolbox, or EXCOGITO, developed to perform a number of operations and analyses on molecular systems pivoting around the properties of mappings. EXCOGITO can process an all-atom trajectory to compute the mapping entropy, identify the mapping that minimizes it, and establish quantitative relations between a low-resolution representation and geometrical, structural, and energetic features of the system. Here, the software, which is available free of charge under an open source licence, is presented and showcased to introduce potential users to its capabilities and usage.","Tue, 12 Mar 2024 22:16:48 UTC (5,482 KB)"
"67","Lessons from a Pioneering Software Engineering Environment: Design Principles of Software through Pictures","Anthony I. (Tony)Wasserman","Software Engineering (cs.SE)","This paper describes the historical background that led to the development of the innovative Software through Pictures multi-user development environment, and the principles for its integration with other software products to create a software engineering environment covering multiple tasks in the software development lifecycle.","Tue, 12 Mar 2024 21:35:08 UTC (1,197 KB)"
"68","BayesFLo: Bayesian fault localization of complex software systems","Yi Ji, Simon Mak, Ryan Lekivetz, Joseph Morgan","Software Engineering (cs.SE)","Software testing is essential for the reliable development of complex software systems. A key step in software testing is fault localization, which uses test data to pinpoint failure-inducing combinations for further diagnosis. Existing fault localization methods, however, are largely deterministic, and thus do not provide a principled approach for assessing probabilistic risk of potential root causes, or for integrating domain and/or structural knowledge from test engineers. To address this, we propose a novel Bayesian fault localization framework called BayesFLo, which leverages a flexible Bayesian model on potential root cause combinations. A key feature of BayesFLo is its integration of the principles of combination hierarchy and heredity, which capture the structured nature of failure-inducing combinations. A critical challenge, however, is the sheer number of potential root cause scenarios to consider, which renders the computation of posterior root cause probabilities infeasible even for small software systems. We thus develop new algorithms for efficient computation of such probabilities, leveraging recent tools from integer programming and graph representations. We then demonstrate the effectiveness of BayesFLo over state-of-the-art fault localization methods, in a suite of numerical experiments and in two motivating case studies on the JMP XGBoost interface.","Tue, 12 Mar 2024 21:12:53 UTC (3,321 KB)"
"69","Towards Code Generation for Octree-Based Multigrid Solvers","Richard Angersbach, Sebastian Kuckuck, Harald Köstler","Computational Engineering, Finance, and Science (cs.CE)","This paper presents a novel method designed to generate multigrid solvers optimized for octree-based software frameworks. Our approach focuses on accurately capturing local features within a domain while leveraging the efficiency inherent in multigrid techniques. We outline the essential steps involved in generating specialized kernels for local refinement and communication routines, integrating on-the-fly interpolations to seamlessly transfer information between refinement levels. For this purpose, we established a software coupling via an automatic fusion of generated multigrid solvers and communication kernels with manual implementations of complex octree data structures and algorithms often found in established software frameworks. We demonstrate the effectiveness of our method through numerical experiments with different interpolation orders. Large-scale benchmarks conducted on the SuperMUC-NG CPU cluster underscore the advantages of our approach, offering a comparison against a reference implementation to highlight the benefits of our method and code generation in general.","Tue, 12 Mar 2024 20:28:00 UTC (737 KB)"
"70","Bus Factor Explorer","Egor Klimov, Muhammad Umair Ahmed, Nikolai Sviridov, Pouria Derakhshanfar, Eray Tüzün, Vladimir Kovalenko","Software Engineering (cs.SE)","Bus factor (BF) is a metric that tracks knowledge distribution in a project. It is the minimal number of engineers that have to leave for a project to stall. Despite the fact that there are several algorithms for calculating the bus factor, only a few tools allow easy calculation of bus factor and convenient analysis of results for projects hosted on Git-based providers. We introduce Bus Factor Explorer, a web application that provides an interface and an API to compute, export, and explore the Bus Factor metric via treemap visualization, simulation mode, and chart editor. It supports repositories hosted on GitHub and enables functionality to search repositories in the interface and process many repositories at the same time. Our tool allows users to identify the files and subsystems at risk of stalling in the event of developer turnover by analyzing the VCS history. The application and its source code are publicly available on GitHub at this https URL. The demonstration video can be found on YouTube: this https URL","Tue, 12 Mar 2024 19:21:48 UTC (10,529 KB)"
"71","Configuration and EMT Simulation of the 240-bus MiniWECC System Integrating Offshore Wind Farms (OWFs)","Buxin She, Hisham Mahmood, Marcelo Elizondo, Veronica Adetola, Yuqing Dong","Systems and Control (eess.SY)","As offshore wind farms (OWFs) become increasingly prevalent in Northern California and Southern Oregon, they introduce faster dynamics into the Western Electricity Coordinating Council (WECC) system, reshaping its dynamic behavior. Accordingly, electromagnetic transient (EMT) simulation is essential to assess high frequency dynamics of the WECC system with integrated OWFs. Against this background, this paper presents the integration of detailed dynamic models of OWFs into a 240-bus miniWECC system in PSCAD software. The sequential initialization technique is employed to facilitate the smooth initiation of a large-scale system in an EMT simulation. The performance of the configured model is assessed under wind speed variations and grounded faults, demonstrating the effectiveness of the miniWECC system with OWFs. This system serves as a valuable basic use case for validating the fast dynamic performance of future WECC systems with high penetration of wind energy.","Tue, 12 Mar 2024 18:00:29 UTC (18,851 KB)"
"72","The XMM Cluster Survey: Automating the estimation of hydrostatic mass for large samples of galaxy clusters I -- Methodology, Validation, & Application to the SDSSRM-XCS sample","D. J. Turner, P. A. Giles, A. K. Romer, J. Pilling, T. K. Lingard, R. Wilkinson, M. Hilton, E. W. Upsdell, R. Al-Serkal, T. Cheng, R. Eappen, P. J. Rooney, S. Bhargava, C. A. Collins, J. Mayers, C. Miller, R. C. Nichol, M. Sahén, P. T. P. Viana","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","We describe features of the X-ray: Generate and Analyse (XGA) open-source software package that have been developed to facilitate automated hydrostatic mass ($M_{\rm hydro}$) measurements from XMM X-ray observations of clusters of galaxies. This includes describing how XGA measures global, and radial, X-ray properties of galaxy clusters. We then demonstrate the reliability of XGA by comparing simple X-ray properties, namely the X-ray temperature and gas mass, with published values presented by the XMM Cluster Survey (XCS), the Ultimate XMM eXtragaLactic survey project (XXL), and the Local Cluster Substructure Survey (LoCuSS). XGA measured values for temperature are, on average, within 1% of the values reported in the literature for each sample. XGA gas masses for XXL clusters are shown to be ${\sim}$10% lower than previous measurements (though the difference is only significant at the $\sim$1.8$\sigma$ level), LoCuSS $R_{2500}$ and $R_{500}$ gas mass re-measurements are 3% and 7% lower respectively (representing a 1.5$\sigma$ and 3.5$\sigma$ difference). Like-for-like comparisons of hydrostatic mass are made to LoCuSS results, which show that our measurements are $10{\pm}3%$ ($19{\pm}7%$) higher for $R_{2500}$ ($R_{500}$). The comparison between $R_{500}$ masses shows significant scatter. Finally, we present new $M_{\rm hydro}$ measurements for 104 clusters from the SDSS DR8 redMaPPer XCS sample (SDSSRM-XCS). Our SDSSRM-XCS hydrostatic mass measurements are in good agreement with multiple literature estimates, and represent one of the largest samples of consistently measured hydrostatic masses. We have demonstrated that XGA is a powerful tool for X-ray analysis of clusters; it will render complex-to-measure X-ray properties accessible to non-specialists.","Tue, 12 Mar 2024 18:00:04 UTC (5,094 KB)"
"73","LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code","Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica","Software Engineering (cs.SE)","Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model","Tue, 12 Mar 2024 17:58:04 UTC (6,890 KB)"
"74","Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition","Geonhwa Jeong, Po-An Tsai, Abhimanyu R. Bambhaniya, Stephen W. Keckler, Tushar Krishna","Machine Learning (cs.LG)","Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and activation tensors so that they can be accelerated by any systems with structured sparse hardware support. Evaluation results show that, by exploiting prior structured sparse hardware baselines, our method can accelerate off-the-shelf dense and sparse DNNs without fine-tuning and improves energy-delay-product by up to 83% and 74% on average.","Tue, 12 Mar 2024 06:25:47 UTC (3,032 KB)"
"75","Advancing Investment Frontiers: Industry-grade Deep Reinforcement Learning for Portfolio Optimization","Philip Ndikum, Serge Ndikum","Artificial Intelligence (cs.AI)","This research paper delves into the application of Deep Reinforcement Learning (DRL) in asset-class agnostic portfolio optimization, integrating industry-grade methodologies with quantitative finance. At the heart of this integration is our robust framework that not only merges advanced DRL algorithms with modern computational techniques but also emphasizes stringent statistical analysis, software engineering and regulatory compliance. To the best of our knowledge, this is the first study integrating financial Reinforcement Learning with sim-to-real methodologies from robotics and mathematical physics, thus enriching our frameworks and arguments with this unique perspective. Our research culminates with the introduction of AlphaOptimizerNet, a proprietary Reinforcement Learning agent (and corresponding library). Developed from a synthesis of state-of-the-art (SOTA) literature and our unique interdisciplinary methodology, AlphaOptimizerNet demonstrates encouraging risk-return optimization across various asset classes with realistic constraints. These preliminary results underscore the practical efficacy of our frameworks. As the finance sector increasingly gravitates towards advanced algorithmic solutions, our study bridges theoretical advancements with real-world applicability, offering a template for ensuring safety and robust standards in this technologically driven future.","Tue, 27 Feb 2024 14:08:31 UTC (6,598 KB)"
"76","Exploring Safety Generalization Challenges of Large Language Models via Code","Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai Lam, Lizhuang Ma","Computation and Language (cs.CL)","The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.","Tue, 12 Mar 2024 17:55:38 UTC (8,284 KB)[v2] Thu, 14 Mar 2024 16:57:37 UTC (8,284 KB)"
"77","Augmenting Interpolation-Based Model Checking with Auxiliary Invariants (Extended Version)","Dirk Beyer, Po-Chun Chien, Nian-Ze Lee","Software Engineering (cs.SE)","Software model checking is a challenging problem, and generating relevant invariants is a key factor in proving the safety properties of a program. Program invariants can be obtained by various approaches, including lightweight procedures based on data-flow analysis and intensive techniques using Craig interpolation. Although data-flow analysis runs efficiently, it often produces invariants that are too weak to prove the properties. By contrast, interpolation-based approaches build strong invariants from interpolants, but they might not scale well due to expensive interpolation procedures. Invariants can also be injected into model-checking algorithms to assist the analysis. Invariant injection has been studied for many well-known approaches, including k-induction, predicate abstraction, and symbolic execution. We propose an augmented interpolation-based verification algorithm that injects external invariants into interpolation-based model checking (McMillan, 2003), a hardware model-checking algorithm recently adopted for software verification. The auxiliary invariants help prune unreachable states in Craig interpolants and confine the analysis to the reachable parts of a program. We implemented the proposed technique in the verification framework CPAchecker and evaluated it against mature SMT-based methods in CPAchecker as well as other state-of-the-art software verifiers. We found that injecting invariants reduces the number of interpolation queries needed to prove safety properties and improves the run-time efficiency. Consequently, the proposed invariant-injection approach verified difficult tasks that none of its plain version (i.e., without invariants), the invariant generator, or any compared tools could solve.","Tue, 12 Mar 2024 17:02:53 UTC (976 KB)"
"78","NPCoronaPredict: A computational pipeline for the prediction of the nanoparticle-biomolecule corona","Ian Rouse, David Power, Julia Subbotina, Vladimir Lobaskin","Mesoscale and Nanoscale Physics (cond-mat.mes-hall)","The corona of a nanoparticle immersed in a biological fluid is of key importance to its eventual fate and bioactivity in the environment or inside live tissues. It is critical to have insight into both the underlying bionano interactions and the corona composition to ensure biocompatibility of novel engineered nanomaterials. A prediction of these properties in silico requires the successful spanning of multiple orders of magnitude of both time and physical dimensions to produce results in a reasonable amount of time, necessitating the development of a multiscale modelling approach. Here, we present the NPCoronaPredict open-source software package: a suite of software tools to enable this prediction for complex multi-component nanomaterials in essentially arbitrary biological fluids, or more generally any medium containing organic molecules. The package integrates several recent physics-based computational models and a library of both physics-based and data-driven parameterisations for nanomaterials and organic molecules. We describe the underlying theoretical background and the package functionality from the design of multi-component NPs through to the evaluation of the corona.","Tue, 12 Mar 2024 17:01:38 UTC (2,793 KB)"
"79","Supporting Error Chains in Static Analysis for Precise Evaluation Results and Enhanced Usability","Anna-Katharina Wickert, Michael Schlichtig, Marvin Vogel, Lukas Winter, Mira Mezini, Eric Bodden","Software Engineering (cs.SE)","Context: Static analyses are well-established to aid in understanding bugs or vulnerabilities during the development process or in large-scale studies. A low false-positive rate is essential for the adaption in practice and for precise results of empirical studies. Unfortunately, static analyses tend to report where a vulnerability manifests rather than the fix location. This can cause presumed false positives or imprecise results. Method: To address this problem, we designed an adaption of an existing static analysis algorithm that can distinguish between a manifestation and fix location, and reports error chains. An error chain represents at least two interconnected errors that occur successively, thus building the connection between the fix and manifestation location. We used our tool CogniCryptSUBS for a case study on 471 GitHub repositories, a performance benchmark to compare different analysis configurations, and conducted an expert interview. Result: We found that 50 % of the projects with a report had at least one error chain. Our runtime benchmark demonstrated that our improvement caused only a minimal runtime overhead of less than 4 %. The results of our expert interview indicate that with our adapted version participants require fewer executions of the analysis. Conclusion: Our results indicate that error chains occur frequently in real-world projects, and ignoring them can lead to imprecise evaluation results. The runtime benchmark indicates that our tool is a feasible and efficient solution for detecting error chains in real-world projects. Further, our results gave a hint that the usability of static analyses may benefit from supporting error chains.","Tue, 12 Mar 2024 16:46:29 UTC (1,112 KB)"
"80","Joint Modeling of Longitudinal Measurements and Time-to-event Outcomes Using BUGS","Taban Baghfalaki, Mojtaba Ganjali, Antoine Barbieri, Reza Hashemi, Hélène Jacqmin-Gadda","Methodology (stat.ME)","The objective of this paper is to provide an introduction to the principles of Bayesian joint modeling of longitudinal measurements and time-to-event outcomes, as well as model implementation using the BUGS language syntax. This syntax can be executed directly using OpenBUGS or by utilizing convenient functions to invoke OpenBUGS and JAGS from R software. In this paper, all details of joint models are provided, ranging from simple to more advanced models. The presentation started with the joint modeling of a Gaussian longitudinal marker and time-to-event outcome. The implementation of the Bayesian paradigm of the model is reviewed. The strategies for simulating data from the JM are also discussed. A proportional hazard model with various forms of baseline hazards, along with the discussion of all possible association structures between the two sub-models are taken into consideration. The paper covers joint models with multivariate longitudinal measurements, zero-inflated longitudinal measurements, competing risks, and time-to-event with cure fraction. The models are illustrated by the analyses of several real data sets. All simulated and real data and code are available at \url{this https URL}.","Tue, 12 Mar 2024 16:02:29 UTC (1,539 KB)"
"81","PROSKILL: A formal skill language for acting in robotics","Félix Ingrand (LAAS-CNRS, Université de Toulouse, Toulouse, France)","Robotics (cs.RO)","Acting is an important decisional function for autonomous robots. Acting relies on skills to implement and to model the activities it oversees: refinement, local recovery, temporal dispatching, external asynchronous events, and commands execution, all done online. While sitting between planning and the robotic platform, acting often relies on programming primitives and an interpreter which executes these skills. Following our experience in providing a formal framework to program the functional components of our robots, we propose a new language, to program the acting skills. This language maps unequivocally into a formal model which can then be used to check properties offline or execute the skills, or more precisely their formal equivalent, and perform runtime verification. We illustrate with a real example how we can program a survey mission for a drone in this new language, prove some formal properties on the program and directly execute the formal model on the drone to perform the mission.","Tue, 12 Mar 2024 15:56:53 UTC (3,571 KB)"
"82","WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?","Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, Alexandre Lacoste","Machine Learning (cs.LG)","We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.","Tue, 12 Mar 2024 14:58:45 UTC (4,544 KB)"
"83","SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt","Edi Sutoyo, Andrea Capiluppi","Software Engineering (cs.SE)","Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances. However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD. In order to address the scarcity of labeled data for SATD \textit{identification} (i.e., whether an instance is SATD or not) and \textit{categorization} (i.e., which type of SATD is being classified) in existing datasets, we share the \textit{SATDAUG} dataset, an augmented version of existing SATD datasets, including source code comments, issue tracker, pull requests, and commit messages. These augmented datasets have been balanced in relation to the available artifacts and provide a much richer source of labeled data for training machine learning or deep learning models.","Tue, 12 Mar 2024 14:33:53 UTC (536 KB)"
"84","Scalable Spatiotemporal Prediction with Bayesian Neural Fields","Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs Köster, Rif A. Saurous, Matthew Hoffman","Machine Learning (cs.LG)","Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements. The paper is accompanied with an open-source software package (this https URL) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform.","Tue, 12 Mar 2024 13:47:50 UTC (11,758 KB)"
"85","A Framework for Controlling Multiple Industrial Robots using Mobile Applications","Daniela Alvarado, Dr. Seemal Asif","Robotics (cs.RO)","Purpose: Over the last few decades, the development of the hardware and software has enabled the application of advanced systems. In the robotics field, the UI design is an intriguing area to be explored due to the creation of devices with a wide range of functionalities in a reduced size. Moreover, the idea of using the same UI to control several systems arouses a great interest considering that this involves less learning effort and time for the users. Therefore, this paper will present a mobile application to control two industrial robots with four modes of operation. Design/methodology/approach: The smartphone was selected to be the interface due to its wide range of capabilities and the MIT Inventor App was used to create the application, whose environment is supported by Android smartphones. For the validation, ROS was used since it is a fundamental framework utilised in industrial robotics and the Arduino Uno was used to establish the data transmission between the smartphone and the board NVIDIA Jetson TX2. In MIT Inventor App, the graphical interface was created to visualize the options available in the app whereas two scripts in python were programmed to perform the simulations in ROS and carry out the tests. Findings: The results indicated that the use of the sliders to control the robots is more favourable than the Orientation Sensor due to the sensibility of the sensor and human limitations to hold the smartphone perfectly still. Another important finding was the limitations of the autonomous mode, in which the robot grabs an object. In this case, the configuration of the Kinect camera and the controllers has a significant impact on the success of the simulation. Finally, it was observed that the delay was appropriate despite the use of the Arduino UNO to transfer the data between the Smartphone and the Nvidia Jetson TX2.","Tue, 12 Mar 2024 13:23:40 UTC (940 KB)"
"86","CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability","Gregory W. Kyro, Matthew T. Martin, Eric D. Watt, Victor S. Batista","Machine Learning (cs.LG)","Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also serve independently as effective components of a virtual screening pipeline. We applied the complete framework to pimozide, an FDA-approved antipsychotic agent that demonstrates high affinity to the hERG channel, and generated 100 refined candidates. Remarkably, among the candidates is fluspirilene, a compound which is of the same class of drugs (diphenylmethanes) as pimozide and therefore has similar pharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We have made all of our software open-source to facilitate integration of the CardioGenAI framework for molecular hypothesis generation into drug discovery workflows.","Tue, 12 Mar 2024 13:12:24 UTC (2,194 KB)"
"87","A Flexible Cell Classification for ML Projects in Jupyter Notebooks","Miguel Perez, Selin Aydin, Horst Lichter","Software Engineering (cs.SE)","Jupyter Notebook is an interactive development environment commonly used for rapid experimentation of machine learning (ML) solutions. Describing the ML activities performed along code cells improves the readability and understanding of Notebooks. Manual annotation of code cells is time-consuming and error-prone. Therefore, tools have been developed that classify the cells of a notebook concerning the ML activity performed in them. However, the current tools are not flexible, as they work based on look-up tables that have been created, which map function calls of commonly used ML libraries to ML activities. These tables must be manually adjusted to account for new or changed libraries. This paper presents a more flexible approach to cell classification based on a hybrid classification approach that combines a rule-based and a decision tree classifier. We discuss the design rationales and describe the developed classifiers in detail. We implemented the new flexible cell classification approach in a tool called JupyLabel. Its evaluation and the obtained metric scores regarding precision, recall, and F1-score are discussed. Additionally, we compared JupyLabel with HeaderGen, an existing cell classification tool. We were able to show that the presented flexible cell classification approach outperforms this tool significantly.","Tue, 12 Mar 2024 11:50:47 UTC (361 KB)"
"88","Process Modeling With Large Language Models","Humam Kourani, Alessandro Berti, Daniel Schuster, Wil M.P. van der Aalst","Software Engineering (cs.SE)","In the realm of Business Process Management (BPM), process modeling plays a crucial role in translating complex process dynamics into comprehensible visual representations, facilitating the understanding, analysis, improvement, and automation of organizational processes. Traditional process modeling methods often require extensive expertise and can be time-consuming. This paper explores the integration of Large Language Models (LLMs) into process modeling to enhance flexibility, efficiency, and accessibility of process modeling for both expert and non-expert users. We propose a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions. Our framework involves innovative prompting strategies for effective LLM utilization, along with a secure model generation protocol and an error-handling mechanism. Moreover, we instantiate a concrete system extending our framework. This system provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations, such as the Business Process Modeling Notation (BPMN) and Petri nets. Preliminary results demonstrate the framework's ability to streamline process modeling tasks, underscoring the transformative potential of generative AI in the BPM field.","Tue, 12 Mar 2024 11:27:47 UTC (432 KB)"
"89","Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code","Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, David Lo","Software Engineering (cs.SE)","Large language models for code (LLM4Code), which demonstrate strong performance (e.g., high accuracy) in processing source code, have significantly transformed software engineering. Many studies separately investigate the non-functional properties of LM4Code, but there is no systematic review of how these properties are evaluated and enhanced. This paper fills this gap by thoroughly examining 146 relevant studies, thereby presenting the first systematic literature review to identify seven important properties beyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability. We discuss the current state-of-the-art methods and trends, identify gaps in existing research, and present promising directions for future study.","Tue, 12 Mar 2024 10:43:26 UTC (608 KB)"
"90","PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates","Janina Schreiber, Pau Batlle, Damar Wicaksono, Michael Hecht","Optimization and Control (math.OC)","We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions. The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization. Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO emerges as the pivotal choice among surrogate-based optimization methods when addressing low-dimensional optimization problems. Hereby, the simple nature of polynomials opens the opportunity for interpretation and analysis of the inferred surrogate model, providing a macroscopic perspective on the landscape of the objective function.","Tue, 12 Mar 2024 10:21:21 UTC (1,861 KB)"
"91","Pedophysics: an open-source python package for soil geophysics","Gaston Mendoza Veirana, Philippe De Smedt, Jeroen Verhegge, Wim Cornelis","Geophysics (physics.geo-ph)","This study introduces Pedophysics, an open-source Python package designed to facilitate solutions for users who work in the field of soil assessment using near-surface geophysical electromagnetic techniques. At the core of this software is the ability to translate geophysical data into specific soil properties (and vice-versa) using pedophysical models (PM). Pedophysical modelling techniques offer valuable insights into various realms including precision agriculture, soil health, resource prospecting, nutrient and land management, hydrogeology, and heritage conservation. In developing a tool for pedophysical modelling, some challenges emerged: selecting suitable PMs from the extensive literature, adapting these to specific conditions, and ensuring adequate data availability. While addressing these, we designed an automated workflow that implements robust PMs (selected after a throughout review), apply different modelling approaches based on soil characteristics and targeted properties, and employs pedotransfer functions and assumptions to integrate missing soil data into PMs. The capabilities of Pedophysics extend to handling complex scenarios such as fusing data from different instruments, incorporating continuous monitoring measurements, and soil calibration data. With these solutions, Pedophysics automates the process of deriving targeted soil and geophysical properties with state-of-art accuracy. Hereby, users can rely on Pedophysics to implement specific knowledge about pedophysical modeling. The software promotes global access to advanced soil geophysical solutions by being open-source and encouraging community contributions. Pedophysics is written in pure Python and has minimal dependencies. It can be easily installed from the Python Package Index (PyPI).","Tue, 12 Mar 2024 10:09:53 UTC (1,066 KB)"
"92","One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices","Marco Chilese, Richard Mitev, Meni Orenbach, Robert Thorburn, Ahmad Atamli, Ahmad-Reza Sadeghi","Cryptography and Security (cs.CR)","Control-Flow Attestation (CFA) is a security service that allows an entity (verifier) to verify the integrity of code execution on a remote computer system (prover). Existing CFA schemes suffer from impractical assumptions, such as requiring access to the prover's internal state (e.g., memory or code), the complete Control-Flow Graph (CFG) of the prover's software, large sets of measurements, or tailor-made hardware. Moreover, current CFA schemes are inadequate for attesting embedded systems due to their high computational overhead and resource usage. In this paper, we overcome the limitations of existing CFA schemes for embedded devices by introducing RAGE, a novel, lightweight CFA approach with minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including control- and non-control-data attacks. It efficiently extracts features from one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to identify deviations from benign executions. The core intuition behind RAGE is to exploit the correspondence between execution trace, execution graph, and execution embeddings to eliminate the unrealistic requirement of having access to a complete CFG. We evaluate RAGE on embedded benchmarks and demonstrate that (i) it detects 40 real-world attacks on embedded software; (ii) Further, we stress our scheme with synthetic return-oriented programming (ROP) and data-oriented programming (DOP) attacks on the real-world embedded software benchmark Embench, achieving 98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive Rate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by millions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP attack detection, with an FPR of 5.47%.","Tue, 12 Mar 2024 10:00:06 UTC (1,447 KB)"
"93","Fixing Smart Contract Vulnerabilities: A Comparative Analysis of Literature and Developer's Practices","Francesco Salzano, Simone Scalabrino, Rocco Oliveto, Remo Pareschi","Software Engineering (cs.SE)","Smart Contracts are programs running logic in the Blockchain network by executing operations through immutable transactions. The Blockchain network validates such transactions, storing them into sequential blocks of which integrity is ensured. Smart Contracts deal with value stakes, if a damaging transaction is validated, it may never be reverted, leading to unrecoverable losses. To prevent this, security aspects have been explored in several fields, with research providing catalogs of security defects, secure code recommendations, and possible solutions to fix vulnerabilities. In our study, we refer to vulnerability fixing in the ways found in the literature as guidelines. However, it is not clear to what extent developers adhere to these guidelines, nor whether there are other viable common solutions and what they are. The goal of our research is to fill knowledge gaps related to developers' observance of existing guidelines and to propose new and viable solutions to security vulnerabilities. To reach our goal, we will obtain from Solidity GitHub repositories the commits that fix vulnerabilities included in the DASP TOP 10 and we will conduct a manual analysis of fixing approaches employed by developers. Our analysis aims to determine the extent to which literature-based fixing strategies are followed. Additionally, we will identify and discuss emerging fixing techniques not currently documented in the literature. Through qualitative analysis, we will evaluate the suitability of these new fixing solutions and discriminate between valid approaches and potential mistakes.","Tue, 12 Mar 2024 09:55:54 UTC (968 KB)"
"94","GPU-Accelerated Vecchia Approximations of Gaussian Processes for Geospatial Data using Batched Matrix Computations","Qilong Pan, Sameh Abdulah, Marc G. Genton, David E. Keyes, Hatem Ltaief, Ying Sun","Computation (stat.CO)","Gaussian processes (GPs) are commonly used for geospatial analysis, but they suffer from high computational complexity when dealing with massive data. For instance, the log-likelihood function required in estimating the statistical model parameters for geospatial data is a computationally intensive procedure that involves computing the inverse of a covariance matrix with size n X n, where n represents the number of geographical locations. As a result, in the literature, studies have shifted towards approximation methods to handle larger values of n effectively while maintaining high accuracy. These methods encompass a range of techniques, including low-rank and sparse approximations. Vecchia approximation is one of the most promising methods to speed up evaluating the log-likelihood function. This study presents a parallel implementation of the Vecchia approximation, utilizing batched matrix computations on contemporary GPUs. The proposed implementation relies on batched linear algebra routines to efficiently execute individual conditional distributions in the Vecchia algorithm. We rely on the KBLAS linear algebra library to perform batched linear algebra operations, reducing the time to solution compared to the state-of-the-art parallel implementation of the likelihood estimation operation in the ExaGeoStat software by up to 700X, 833X, 1380X on 32GB GV100, 80GB A100, and 80GB H100 GPUs, respectively. We also successfully manage larger problem sizes on a single NVIDIA GPU, accommodating up to 1M locations with 80GB A100 and H100 GPUs while maintaining the necessary application accuracy. We further assess the accuracy performance of the implemented algorithm, identifying the optimal settings for the Vecchia approximation algorithm to preserve accuracy on two real geospatial datasets: soil moisture data in the Mississippi Basin area and wind speed data in the Middle East.","Tue, 12 Mar 2024 08:39:08 UTC (7,019 KB)"
"95","Time-Efficient Light-Field Acquisition Using Coded Aperture and Events","Shuji Habuchi, Keita Takahashi, Chihiro Tsutake, Toshiaki Fujii, Hajime Nagahara","Computer Vision and Pattern Recognition (cs.CV)","We propose a computational imaging method for time-efficient light-field acquisition that combines a coded aperture with an event-based camera. Different from the conventional coded-aperture imaging method, our method applies a sequence of coding patterns during a single exposure for an image frame. The parallax information, which is related to the differences in coding patterns, is recorded as events. The image frame and events, all of which are measured in a single exposure, are jointly used to computationally reconstruct a light field. We also designed an algorithm pipeline for our method that is end-to-end trainable on the basis of deep optics and compatible with real camera hardware. We experimentally showed that our method can achieve more accurate reconstruction than several other imaging methods with a single exposure. We also developed a hardware prototype with the potential to complete the measurement on the camera within 22 msec and demonstrated that light fields from real 3-D scenes can be obtained with convincing visual quality. Our software and supplementary video are available from our project website.","Tue, 12 Mar 2024 02:04:17 UTC (16,849 KB)"
"96","Towards Full Automation of Geometry Extraction for Biomechanical Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical Methodologies","Farah Alkhatib, Mostafa Jamshidian, Donatien Le Liepvre, Florian Bernard, Ludovic Minvielle, Adam Wittek, Karol Miller","Computational Engineering, Finance, and Science (cs.CE)","In this study we investigated the impact of image segmentation methods on the results of stress computation in the wall of abdominal aortic aneurysms (AAAs). We compared wall stress distributions and magnitudes calculated from geometry models obtained from classical semi-automated segmentation versus automated neural network-based segmentation. Ten different AAA contrast-enhanced computed tomography (CT) images were semi-automatically segmented by an analyst, taking, depending on the quality of an image, between 15 and 40 minutes of human effort per patient. The same images were automatically segmented using PRAEVAorta 2, commercial software by NUREA (this https URL), developed based on artificial intelligence (AI) algorithms, requiring only 1-2 minutes of computer time per patient. Aneurysm wall stress calculations performed using the BioPARR software (this https URL) revealed that, compared to the classical semi-automated segmentation, the automatic neural network-based segmentation leads to equivalent stress distributions, and slightly higher peak and 99th percentile maximum principal stress values. This difference is due to consistently larger lumen surface areas in automatically segmented models as compared to classical semi-automated segmentations, resulting in greater total pressure load on the wall. Our findings are a steppingstone toward a fully automated pipeline for biomechanical analysis of AAAs, starting with CT scans and concluding with wall stress assessment, while at the same time highlighting the critical importance of the repeatable and accurate segmentation of the lumen, the difficult problem often underestimated by the literature.","Tue, 12 Mar 2024 01:20:34 UTC (3,668 KB)"
"97","Digital Twin Evolution for Sustainable Smart Ecosystems","Istvan David, Judith Michael, Dominik Bork","Software Engineering (cs.SE)","Smart ecosystems are the drivers of modern society. They control critical infrastructures, ensuring their stable and sustainable operation. Smart ecosystems are governed by digital twins -- real-time virtual representations of physical infrastructure. To support the open-ended and reactive traits of smart ecosystems, digital twins need to be able to evolve in reaction to changing conditions. However, digital twin evolution is particularly challenging due to the intertwined nature of physical and software components. As a consequence, software practitioners find a substantial body of knowledge on software evolution hard to apply in digital twin evolution scenarios. In this article, we provide software practitioners with tangible leads toward understanding and managing the evolutionary concerns of digital twins. By that, we aim to bridge a significant gap in leveraging software engineering practices to develop robust smart ecosystems.","Mon, 11 Mar 2024 21:06:13 UTC (4,755 KB)"
"98","Better than classical? The subtle art of benchmarking quantum machine learning models","Joseph Bowles, Shahnawaz Ahmed, Maria Schuld","Quantum Physics (quant-ph)","Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that ""quantumness"" may not be the crucial ingredient for the small learning tasks considered here. Our benchmarks also unlock investigations beyond simplistic leaderboard comparisons, and we identify five important questions for quantum model design that follow from our results.","Mon, 11 Mar 2024 18:00:06 UTC (8,528 KB)[v2] Thu, 14 Mar 2024 11:02:26 UTC (8,534 KB)"
"99","Comparison of Static Analysis Architecture Recovery Tools for Microservice Applications","Simon Schneider, Alexander Bakhtin, Xiaozhou Li, Jacopo Soldani, Antonio Brogi, Tomas Cerny, Riccardo Scandariato, Davide Taibi","Software Engineering (cs.SE)","Architecture recovery tools help software engineers obtain an overview of their software systems during all phases of the software development lifecycle. This is especially important for microservice applications because their distributed nature makes it more challenging to oversee the architecture. Various tools and techniques for this task are presented in academic and grey literature sources. Practitioners and researchers can benefit from a comprehensive overview of these tools and their abilities. However, no such overview exists that is based on executing the identified tools and assessing their outputs regarding effectiveness. With the study described in this paper, we plan to first identify static analysis architecture recovery tools for microservice applications via a multi-vocal literature review, and then execute them on a common dataset and compare the measured effectiveness in architecture recovery. We will focus on static approaches because they are also suitable for integration into fast-paced CI/CD pipelines.","Mon, 11 Mar 2024 17:26:51 UTC (124 KB)"
"100","Numerical simulation of individual coil placement -- A proof-of-concept study for the prediction of recurrence after aneurysm coiling","Julian Schwarting, Fabian Holzberger, Markus Muhr, Martin Renz, Tobias Boeckh-Behrens, Barbara Wohlmuth, Jan Kirschke","Computational Engineering, Finance, and Science (cs.CE)","Rupture of intracranial aneurysms results in severe subarachnoidal hemorrhage, which is associated with high morbidity and mortality. Neurointerventional occlusion of the aneurysm through coiling has evolved to a therapeutical standard. The choice of the specific coil has an important influence on secondary regrowth requiring retreatment. Aneurysm occlusion was simulated either through virtual implantation of a preshaped 3D coil or with a porous media approach. In this study, we used a recently developed numerical approach to simulate aneurysm shapes in specific challenging aneurysm anatomies and correlated these with aneurysm recurrence 6 months after treatment. The simulation showed a great variety of coil shapes depending on the variability in possible microcatheter positions. Aneurysms with a later recurrence showed a tendency for more successful coiling attempts. Results revealed further trends suggesting lower simulated packing densities in aneurysms with reoccurrence. Simulated packing densities did not correlate with those calculated by conventional software, indicating the potential for our approach to offer additional predictive value. Our study, therefore, pioneers a comprehensive numerical model for simulating aneurysm coiling, providing insights into individualized treatment strategies and outcome prediction. Future directions involve expanding the model's capabilities to simulate intraprocedural outcomes and long-term predictions, aiming to refine occlusion quality criteria and validate prediction parameters in larger patient cohorts. This simulation framework holds promise for enhancing clinical decision-making and optimizing patient outcomes in endovascular aneurysm treatment.","Mon, 11 Mar 2024 16:45:53 UTC (605 KB)"
"101","SonoTraceLab -- A Raytracing-Based Acoustic Modelling System for Simulating Echolocation Behavior of Bats","Wouter Jansen, Jan Steckel","Audio and Speech Processing (eess.AS)","Echolocation is the prime sensing modality for many species of bats, who show the intricate ability to perform a plethora of tasks in complex and unstructured environments. Understanding this exceptional feat of sensorimotor interaction is a key aspect into building more robust and performant man-made sonar sensors. In order to better understand the underlying perception mechanisms it is important to get a good insight into the nature of the reflected signals that the bat perceives. While ensonification experiments are in important way to better understand the nature of these signals, they are as time-consuming to perform as they are informative. In this paper we present SonoTraceLab, an open-source software package for simulating both technical as well as biological sonar systems in complex scenes. Using simulation approaches can drastically increase insights into the nature of biological echolocation systems, while reducing the time- and material complexity of performing them.","Mon, 11 Mar 2024 16:04:18 UTC (25,588 KB)"
"102","Inverse Garment and Pattern Modeling with a Differentiable Simulator","Boyang Yu, Frederic Cordier, Hyewon Seo","Graphics (cs.GR)","The capability to generate simulation-ready garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured geometry of real garments, as well as their faithful reproduction in the virtual world. This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry. To align with the garment modeling process standardized by the fashion industry as well as cloth simulation softwares, it is required to recover 2D patterns. This involves an inverse garment design problem, which is the focus of our work here: Starting with an arbitrary target garment geometry, our system estimates an animatable garment model by automatically adjusting its corresponding 2D template pattern, along with the material parameters of the physics-based simulation (PBS). Built upon a differentiable cloth simulator, the optimization process is directed towards minimizing the deviation of the simulated garment shape from the target geometry. Moreover, our produced patterns meet manufacturing requirements such as left-to-right-symmetry, making them suited for reverse garment fabrication. We validate our approach on examples of different garment types, and show that our method faithfully reproduces both the draped garment shape and the sewing pattern.","Mon, 11 Mar 2024 16:01:07 UTC (19,609 KB)[v2] Fri, 15 Mar 2024 13:34:37 UTC (11,257 KB)"
"103","ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts","Lyuye Zhang, Kaixuan Li, Kairan Sun, Daoyuan Wu, Ye Liu, Haoye Tian, Yang Liu","Software Engineering (cs.SE)","Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, the automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of AC lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence. Leveraging recent advancements in large language models (LLMs), we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX. The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality. To this end, ACFIX involves both offline and online phases. First, during the offline phase, ACFIX mines a taxonomy of common Role-based Access Control (RBAC) practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 pairs mined. Second, during the online phase, ACFIX tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch. This patch will then undergo a validity and effectiveness check. To evaluate ACFIX, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFIX successfully repaired 94.92% of them. This represents a significant improvement compared to the baseline GPT-4, which achieved only 52.54%.","Mon, 11 Mar 2024 15:59:59 UTC (5,332 KB)"
"104","PolyominoIdeals: a package for Macaulay2 to work with the inner $2$-minor ideals of collections of cells","Cisto Carmelo, Rizwan Jahangir, Francesco Navarra","Commutative Algebra (math.AC)","Let $\mathcal{P}$ be a collection of cells and $I_\mathcal{P}$ be the associated ideal of inner $2$-minors as defined by A. A. Qureshi in 2012. In this paper, we provide a description of the package $\texttt{PolyominoIdeals}$ for the computer algebra software $\texttt{Macaulay2}$. More precisely, this package provides some functions that allow to define the ideal $I_{\mathcal{P}}$ in $\texttt{Macaulay2}$ and to compute its algebraic invariants or verifying its algebraic properties. We explain the usage of these functions and also give some examples.","Mon, 11 Mar 2024 14:10:08 UTC (1,419 KB)"
"105","A SysML Profile for the Standardized Description of Processes during System Development","Lasse Beers, Hamied Nabizada, Maximilian Weigand, Felix Gehlhoff, Alexander Fay","Software Engineering (cs.SE)","A key aspect in creating models of production systems with the use of model-based systems engineering (MBSE) lies in the description of system functions. These functions shouldbe described in a clear and standardized manner.The VDI/VDE 3682 standard for Formalised Process De-scription (FPD) provides a simple and easily understandable representation of processes. These processes can be conceptualized as functions within the system model, making the FPD particularly well-suited for the standardized representation ofthe required functions. Hence, this contribution focuses on thedevelopment of a Domain-Specific Modeling Language(DSML) that facilitates the integration of VDI/VDE 3682 into the Systems Modeling Language (SysML). The presented approach not onlyextends classical SysML with domain-specific requirements but also facilitates model verification through constraints modeled in Object Constraint Language (OCL). Additionally, it enables automatic serialization of process descriptions into the Extensible Markup Language (XML) using the Velocity Template Language (VTL). This serialization enables the use of process modeling in applications outside of MBSE. The approach was validated using an collar screwing use case in the major component assembly in aircraft production.","Mon, 11 Mar 2024 13:44:38 UTC (2,624 KB)"
"106","NLP4RE Tools: Classification, Overview, and Management","Julian Frattini, Michael Unterkalmsteiner, Davide Fucci, Daniel Mendez","Software Engineering (cs.SE)","Tools constitute an essential contribution to natural language processing for requirements engineering (NLP4RE) research. They are executable instruments that make research usable and applicable in practice. In this chapter, we first introduce a systematic classification of NLP4RE tools to improve the understanding of their types and properties. Then, we extend an existing overview with a systematic summary of 126 NLP4RE tools published between April 2019 and June 2023 to ease reuse and evolution of existing tools. Finally, we provide instructions on how to create, maintain, and disseminate NLP4RE tools to support a more rigorous management and dissemination.","Mon, 11 Mar 2024 13:01:57 UTC (73 KB)"
"107","Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code","Cristina Improta","Cryptography and Security (cs.CR)","AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation. Lastly, we discuss potential solutions to overcome this threat.","Mon, 11 Mar 2024 12:47:04 UTC (963 KB)"
"108","SmartML: Towards a Modeling Language for Smart Contracts","Adele Veschetti, Richard Bubel, Reiner Hähnle","Software Engineering (cs.SE)","Smart contracts codify real-world transactions and automatically execute the terms of the contract when predefined conditions are met. This paper proposes SmartML, a modeling language for smart contracts that is platform independent and easy to comprehend. We detail the formal semantics and the type system, focusing on its role in addressing security vulnerabilities and attacks. Through case studies we show how SmartML contributes to the prevention of reentrancy attacks, illustrating its efficacy in reinforcing the reliability and security of smart contracts within decentralized systems.","Mon, 11 Mar 2024 11:27:53 UTC (456 KB)"
"109","Asset-driven Threat Modeling for AI-based Systems","Jan von der Assen, Jamo Sharif, Chao Feng, Gérôme Bovet, Burkhard Stiller","Cryptography and Security (cs.CR)","Threat modeling is a popular method to securely develop systems by achieving awareness of potential areas of future damage caused by adversaries. The benefit of threat modeling lies in its ability to indicate areas of concern, paving the way to consider mitigation during the design stage. However, threat modeling for systems relying on Artificial Intelligence is still not well explored. While conventional threat modeling methods and tools did not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice. To evaluate that the work at hand is able to guide and automatically identify AI-related threats during the architecture definition stage, several experts were tasked to create a threat model of an AI system designed in the healthcare domain. The usability of the solution was well-perceived, and the results indicate that it is effective for threat identification.","Mon, 11 Mar 2024 08:40:01 UTC (1,170 KB)"
"110","QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning","Jiun-Man Chen, Yu-Hsuan Chao, Yu-Jie Wang, Ming-Der Shieh, Chih-Chung Hsu, Wei-Fen Lin","Computer Vision and Pattern Recognition (cs.CV)","Transformer-based models have gained widespread popularity in both the computer vision (CV) and natural language processing (NLP) fields. However, significant challenges arise during post-training linear quantization, leading to noticeable reductions in inference accuracy. Our study focuses on uncovering the underlying causes of these accuracy drops and proposing a quantization-friendly fine-tuning method, \textbf{QuantTune}. Firstly, our analysis revealed that, on average, 65\% of quantization errors result from the precision loss incurred by the dynamic range amplification effect of outliers across the target Transformer-based models. Secondly, \textbf{QuantTune} adjusts weights based on the deviation of outlier activations and effectively constrains the dynamic ranges of the problematic activations. As a result, it successfully mitigates the negative impact of outliers on the inference accuracy of quantized models. Lastly, \textbf{QuantTune} can be seamlessly integrated into the back-propagation pass in the fine-tuning process without requiring extra complexity in inference software and hardware design. Our approach showcases significant improvements in post-training quantization across a range of Transformer-based models, including ViT, Bert-base, and OPT. QuantTune reduces accuracy drops by 12.09\% at 8-bit quantization and 33.8\% at 7-bit compared to top calibration methods, outperforming state-of-the-art solutions by over 18.84\% across ViT models.","Mon, 11 Mar 2024 08:09:30 UTC (2,879 KB)"
"111","Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach","Jinxi Kuang, Jinyang Liu, Junjie Huang, Renyi Zhong, Jiazhen Gu, Lan Yu, Rui Tan, Zengyin Yang, Michael R. Lyu","Software Engineering (cs.SE)","Due to the scale and complexity of cloud systems, a system failure would trigger an ""alert storm"", i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts. To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation. The correlation mining module effectively captures the temporal and spatial relations between alerts, measuring their correlations in an efficient manner. Subsequently, only uncertain pairs with low confidence are forwarded to the LLM reasoning module for detailed analysis. This hybrid design harnesses both statistical evidence for frequent alerts and the reasoning capabilities of computationally intensive LLMs, ensuring the overall efficiency of COLA in handling large volumes of alerts in practical scenarios. We evaluate COLA on three datasets collected from the production environment of a large-scale cloud platform. The experimental results show COLA achieves F1-scores from 0.901 to 0.930, outperforming state-of-the-art methods and achieving comparable efficiency. We also share our experience in deploying COLA in our real-world cloud system, Cloud X.","Mon, 11 Mar 2024 07:48:35 UTC (1,921 KB)"
"112","Technical Debt Management: The Road Ahead for Successful Software Delivery","Paris Avgeriou, Ipek Ozkaya, Alexander Chatzigeorgiou, Marcus Ciolkowski, Neil A. Ernst, Ronald J. Koontz, Eltjo Poort, Forrest Shull","Software Engineering (cs.SE)","Technical Debt, considered by many to be the 'silent killer' of software projects, has undeniably become part of the everyday vocabulary of software engineers. We know it compromises the internal quality of a system, either deliberately or inadvertently. We understand Technical Debt is not all derogatory, often serving the purpose of expediency. But, it is associated with a clear risk, especially for large and complex systems with extended service life: if we do not properly manage Technical Debt, it threatens to ""bankrupt"" those systems. Software engineers and organizations that develop software-intensive systems are facing an increasingly more dire future state of those systems if they do not start incorporating Technical Debt management into their day to day practice. But how? What have the wins and losses of the past decade of research and practice in managing Technical Debt taught us and where should we focus next? In this paper, we examine the state of the art in both industry and research communities in managing Technical Debt; we subsequently distill the gaps in industrial practice and the research shortcomings, and synthesize them to define and articulate a vision for what Technical Debt management looks like five years hence.","Mon, 11 Mar 2024 07:48:35 UTC (103 KB)"
"113","Exploring Hardware Friendly Bottleneck Architecture in CNN for Embedded Computing Systems","Xing Lei, Longjun Liu, Zhiheng Zhou, Hongbin Sun, Nanning Zheng","Computer Vision and Pattern Recognition (cs.CV)","In this paper, we explore how to design lightweight CNN architecture for embedded computing systems. We propose L-Mobilenet model for ZYNQ based hardware platform. L-Mobilenet can adapt well to the hardware computing and accelerating, and its network structure is inspired by the state-of-the-art work of Inception-ResnetV1 and MobilenetV2, which can effectively reduce parameters and delay while maintaining the accuracy of inference. We deploy our L-Mobilenet model to ZYNQ embedded platform for fully evaluating the performance of our design. By measuring in cifar10 and cifar100 datasets, L-Mobilenet model is able to gain 3x speed up and 3.7x fewer parameters than MobileNetV2 while maintaining a similar accuracy. It also can obtain 2x speed up and 1.5x fewer parameters than ShufflenetV2 while maintaining the same accuracy. Experiments show that our network model can obtain better performance because of the special considerations for hardware accelerating and software-hardware co-design strategies in our L-Mobilenet bottleneck architecture.","Mon, 11 Mar 2024 01:02:01 UTC (470 KB)"
"114","Cross-ecosystem categorization: A manual-curation protocol for the categorization of Java Maven libraries along Python PyPI Topics","Ranindya Paramitha, Yuan Feng, Fabio Massacci, Carlos E. Budde","Software Engineering (cs.SE)","Context: Software of different functional categories, such as text processing vs. networking, has different profiles in terms of metrics like security and updates. Using popularity to compare e.g. Java vs. Python libraries might give a skewed perspective, as the categories of the most popular software vary from one ecosystem to the next. How can one compare libraries datasets across software ecosystems, when not even the category names are uniform among them? Objective: We study how to generate a language-agnostic categorisation of software by functional purpose, that enables cross-ecosystem studies of libraries datasets. This provides the functional fingerprint information needed for software metrics comparisons. Method: We designed and implemented a human-guided protocol to categorise libraries from software ecosystems. Category names mirror PyPI Topic classifiers, but the protocol is generic and can be applied to any ecosystem. We demonstrate it by categorising 256 Java/Maven libraries with severe security vulnerabilities. Results: The protocol allows three or more people to categorise any number of libraries. The categorisation produced is functional-oriented and language-agnostic. The Java/Maven dataset demonstration resulted in a majority of Internet-oriented libraries, coherent with its selection by severe vulnerabilities. To allow replication and updates, we make the dataset and the protocol individual steps available as open data. Conclusions: Libraries categorisation by functional purpose is feasible with our protocol, which produced the fingerprint of a 256-libraries Java dataset. While this was labour intensive, humans excel in the required inference tasks, so full automation of the process is not envisioned. However, results can provide the ground truth needed for machine learning in large-scale cross-ecosystem empirical studies.","Sun, 10 Mar 2024 20:15:08 UTC (489 KB)"
"115","LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard's Capacity to Handle Object-Oriented Programming Assignments","Bruno Pereira Cipriano, Pedro Alves","Software Engineering (cs.SE)","Large Language Models (LLMs) have emerged as promising tools to assist students while solving programming assignments. However, object-oriented programming (OOP), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. Contrary to introductory programming exercises, there exists a research gap with regard to the behavior of LLMs in OOP contexts. In this study, we experimented with three prominent LLMs - GPT-3.5, GPT-4, and Bard - to solve real-world OOP exercises used in educational settings, subsequently validating their solutions using an Automatic Assessment Tool (AAT). The findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of OOP. GPT-4 stood out as the most proficient, followed by GPT-3.5, with Bard trailing last. We advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing LLMs with AATs in pedagogical settings. In conclusion, while GPT-4 showcases promise, the deployment of these models in OOP education still mandates supervision.","Sun, 10 Mar 2024 16:40:05 UTC (579 KB)"
"116","I/O Transit Caching for PMem-based Block Device","Qing Xu, Qisheng Jiang, Chundong Wang","Hardware Architecture (cs.AR)","Byte-addressable non-volatile memory (NVM) sitting on the memory bus is employed to make persistent memory (PMem) in general-purpose computing systems and embedded systems for data storage. Researchers develop software drivers such as the block translation table (BTT) to build block devices on PMem, so programmers can keep using mature and reliable conventional storage stack while expecting high performance by exploiting fast PMem. However, our quantitative study shows that BTT underutilizes PMem and yields inferior performance, due to the absence of the imperative in-device cache. We add a conventional I/O staging cache made of DRAM space to BTT. As DRAM and PMem have comparable access latency, I/O staging cache is likely to be fully filled over time. Continual cache evictions and fsyncs thus cause on-demand flushes with severe stalls, such that the I/O staging cache is concretely unappealing for PMem-based block devices. We accordingly propose an algorithm named Caiti with novel I/O transit caching. Caiti eagerly evicts buffered data to PMem through CPU's multi-cores. It also conditionally bypasses a full cache and directly writes data into PMem to further alleviate I/O stalls. Experiments confirm that Caiti significantly boosts the performance with BTT by up to 3.6x, without loss of block-level write atomicity.","Sun, 10 Mar 2024 07:35:31 UTC (8,283 KB)"
"117","RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion","Huy N. Phan, Hoang N. Phan, Tien N. Nguyen, Nghi D. Q. Bui","Software Engineering (cs.SE)","Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in repository-level code completion, showcasing enhanced accuracy across various datasets when compared to several strong baselines.","Sun, 10 Mar 2024 05:10:34 UTC (856 KB)"
"118","Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills","Paul Denny, David H. Smith IV, Max Fowler, James Prather, Brett A. Becker, Juho Leinonen","Human-Computer Interaction (cs.HC)","Reading, understanding and explaining code have traditionally been important skills for novices learning programming. As large language models (LLMs) become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. Brand new skills are also needed, such as the ability to formulate clear prompts that can elicit intended code from an LLM. Thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with LLMs. One effective way to develop and assess code comprehension ability is with ``Explain in plain English'' (EiPE) questions, where students succinctly explain the purpose of a fragment of code. However, grading EiPE questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. In this paper, we explore a natural synergy between EiPE questions and code-generating LLMs to overcome this limitation. We propose using an LLM to generate code based on students' responses to EiPE questions -- not only enabling EiPE responses to be assessed automatically, but helping students develop essential code comprehension and prompt crafting skills in parallel. We investigate this idea in an introductory programming course and report student success in creating effective prompts for solving EiPE questions. We also examine student perceptions of this activity and how it influences their views on the use of LLMs for aiding and assessing learning.","Sun, 10 Mar 2024 00:23:08 UTC (192 KB)"
"119","A Tool for Automated Reasoning About Traces Based on Configurable Formal Semantics","Ferhat Erata, Arda Goknil, Bedir Tekinerdogan, Geylani Kardas","Software Engineering (cs.SE)","We present Tarski, a tool for specifying configurable trace semantics to facilitate automated reasoning about traces. Software development projects require that various types of traces be modeled between and within development artifacts. For any given artifact (e.g., requirements, architecture models and source code), Tarski allows the user to specify new trace types and their configurable semantics, while, using the semantics, it automatically infers new traces based on existing traces provided by the user, and checks the consistency of traces. It has been evaluated on three industrial case studies in the automotive domain (this https URL).","Sat, 9 Mar 2024 21:23:40 UTC (2,860 KB)"
"120","Integrating Static Code Analysis Toolchains","Matthias Kern, Ferhat Erata, Markus Iser, Carsten Sinz, Frederic Loiret, Stefan Otten, Eric Sax","Software Engineering (cs.SE)","This paper proposes an approach for a tool-agnostic and heterogeneous static code analysis toolchain in combination with an exchange format. This approach enhances both traceability and comparability of analysis results. State of the art toolchains support features for either test execution and build automation or traceability between tests, requirements and design information. Our approach combines all those features and extends traceability to the source code level, incorporating static code analysis. As part of our approach we introduce the ""ASSUME Static Code Analysis tool exchange format"" that facilitates the comparability of different static code analysis results. We demonstrate how this approach enhances the usability and efficiency of static code analysis in a development process. On the one hand, our approach enables the exchange of results and evaluations between static code analysis tools. On the other hand, it enables a complete traceability between requirements, designs, implementation, and the results of static code analysis. Within our approach we also propose an OSLC specification for static code analysis tools and an OSLC communication framework.","Sat, 9 Mar 2024 18:59:50 UTC (984 KB)"
"121","A Novel Refactoring and Semantic Aware Abstract Syntax Tree Differencing Tool and a Benchmark for Evaluating the Accuracy of Diff Tools","Pouria Alikhanifard, Nikolaos Tsantalis","Software Engineering (cs.SE)","Software undergoes constant changes to support new requirements, address bugs, enhance performance, and ensure maintainability. Thus, developers spend a great portion of their workday trying to understand and review the code changes of their teammates. Abstract Syntax Tree (AST) diff tools were developed to overcome the limitations of line-based diff tools, which are used by the majority of developers. Despite the notable improvements brought by AST diff tools in understanding complex changes, they still suffer from serious limitations, such as (1) lacking multi-mapping support, (2) matching semantically incompatible AST nodes, (3) ignoring language clues to guide the matching process, (4) lacking refactoring awareness, and (5) lacking commit-level diff support. We propose a novel AST diff tool based on RefactoringMiner that resolves all aforementioned limitations. First, we improved RefactoringMiner to increase its statement mapping accuracy, and then we developed an algorithm that generates AST diff for a given commit or pull request based on the refactoring instances and pairs of matched program element declarations provided by RefactoringMiner. To evaluate the accuracy of our tool and compare it with the state-of-the-art tools, we created the first benchmark of AST node mappings, including 800 bug-fixing commits and 188 refactoring commits. Our evaluation showed that our tool achieved a considerably higher precision and recall, especially for refactoring commits, with an execution time that is comparable with that of the faster tools.","Sat, 9 Mar 2024 15:32:41 UTC (1,345 KB)"
"122","LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss","Yen-Trang Dang, Thanh-Le Cong, Phuc-Thanh Nguyen, Anh M. T. Bui, Phuong T. Nguyen, Bach Le, Quyet-Thang Huynh","Software Engineering (cs.SE)","Open-source development has revolutionized the software industry by promoting collaboration, transparency, and community-driven innovation. Today, a vast amount of various kinds of open-source software, which form networks of repositories, is often hosted on GitHub - a popular software development platform. To enhance the discoverability of the repository networks, i.e., groups of similar repositories, GitHub introduced repository topics in 2017 that enable users to more easily explore relevant projects by type, technology, and more. It is thus crucial to accurately assign topics for each GitHub repository. Current methods for automatic topic recommendation rely heavily on TF-IDF for encoding textual data, presenting challenges in understanding semantic nuances. This paper addresses the limitations of existing techniques by proposing Legion, a novel approach that leverages Pre-trained Language Models (PTMs) for recommending topics for GitHub repositories. The key novelty of Legion is three-fold. First, Legion leverages the extensive capabilities of PTMs in language understanding to capture contextual information and semantic meaning in GitHub repositories. Second, Legion overcomes the challenge of long-tailed distribution, which results in a bias toward popular topics in PTMs, by proposing a Distribution-Balanced Loss (DB Loss) to better train the PTMs. Third, Legion employs a filter to eliminate vague recommendations, thereby improving the precision of PTMs. Our empirical evaluation on a benchmark dataset of real-world GitHub repositories shows that Legion can improve vanilla PTMs by up to 26% on recommending GitHubs topics. Legion also can suggest GitHub topics more precisely and effectively than the state-of-the-art baseline with an average improvement of 20% and 5% in terms of Precision and F1-score, respectively.","Sat, 9 Mar 2024 10:49:31 UTC (1,424 KB)"
"123","Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations","Kuan-Cheng Chen, Xiaoren Li, Xiaotian Xu, Yun-Yuan Wang, Chen-Yu Liu","Quantum Physics (quant-ph)","Achieving high-performance computation on quantum systems presents a formidable challenge that necessitates bridging the capabilities between quantum hardware and classical computing resources. This study introduces an innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture, which integrates cutting-edge quantum software framework works with high-performance classical computing resources to address challenges in quantum simulation for materials and condensed matter physics. At the heart of this architecture is the seamless integration of VQE algorithms running on QPUs for efficient quantum state preparation, Tensor Network states, and QCNNs for classifying quantum states on classical hardware. For benchmarking quantum simulators, the QCQ architecture utilizes the cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's Lightning plugin, demonstrating up to tenfold increases in computational speed for complex phase transition classification tasks compared to traditional CPU-based methods. This significant acceleration enables models such as the transverse field Ising and XXZ systems to accurately predict phase transitions with a 99.5% accuracy. The architecture's ability to distribute computation between QPUs and classical resources addresses critical bottlenecks in Quantum-HPC, paving the way for scalable quantum simulation. The QCQ framework embodies a synergistic combination of quantum algorithms, machine learning, and Quantum-HPC capabilities, enhancing its potential to provide transformative insights into the behavior of quantum systems across different scales. As quantum hardware continues to improve, this hybrid distribution-aware framework will play a crucial role in realizing the full potential of quantum computing by seamlessly integrating distributed quantum resources with the state-of-the-art classical computing infrastructure.","Sat, 9 Mar 2024 07:38:45 UTC (2,016 KB)"
"124","UniSparse: An Intermediate Language for General Sparse Format Customization","Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, Zhiru Zhang","Computation and Language (cs.CL)","The ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. These formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. Unlike the existing attribute-based frameworks, UniSparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-level memory layout, enabling the customization of both. As a result, a rich set of format customizations can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with specialized formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory (PIM) device.","Sat, 9 Mar 2024 05:38:45 UTC (1,733 KB)"
"125","Numerical cubature and hyperinterpolation over Spherical Polygons","Alvise Sommariva","Numerical Analysis (math.NA)","The purpose of this work is to introduce a strategy for determining the nodes and weights of a low-cardinality positive cubature formula nearly exact for polynomials of a given degree over spherical polygons. In the numerical section we report the results about numerical cubature over a spherical polygon $\cal P$ approximating Australia and reconstruction of functions over such $\cal P$, also affected by perturbations, via hyperinterpolation and some of its variants. The open-source Matlab software used in the numerical tests is available at the author's homepage.","Sat, 9 Mar 2024 00:07:24 UTC (1,240 KB)"
"126","Engineering Formality and Software Risk in Debian Python Packages","Matthew Gaughan, Kaylea Champion, Sohyeon Hwang","Software Engineering (cs.SE)","While free/libre and open source software (FLOSS) is critical to global computing infrastructure, the maintenance of widely-adopted FLOSS packages is dependent on volunteer developers who select their own tasks. Risk of failure due to the misalignment of engineering supply and demand -- known as underproduction -- has led to code base decay and subsequent cybersecurity incidents such as the Heartbleed and Log4Shell vulnerabilities. FLOSS projects are self-organizing but can often expand into larger, more formal efforts. Although some prior work suggests that becoming a more formal organization decreases project risk, other work suggests that formalization may increase the likelihood of project abandonment. We evaluate the relationship between underproduction and formality, focusing on formal structure, developer responsibility, and work process management. We analyze 182 packages written in Python and made available via the Debian GNU/Linux distribution. We find that although more formal structures are associated with higher risk of underproduction, more elevated developer responsibility is associated with less underproduction, and the relationship between formal work process management and underproduction is not statistically significant. Our analysis suggests that a FLOSS organization's transformation into a more formal structure may face unintended consequences which must be carefully managed.","Fri, 8 Mar 2024 23:44:01 UTC (271 KB)"
"127","Digital Wellbeing Redefined: Toward User-Centric Approach for Positive Social Media Engagement","Yixue Zhao, Tianyi Li, Michael Sobolev","Human-Computer Interaction (cs.HC)","The prevalence of social media and its escalating impact on mental health has highlighted the need for effective digital wellbeing strategies. Current digital wellbeing interventions have primarily focused on reducing screen time and social media use, often neglecting the potential benefits of these platforms. This paper introduces a new perspective centered around empowering positive social media experiences, instead of limiting users with restrictive rules. In line with this perspective, we lay out the key requirements that should be considered in future work, aiming to spark a dialogue in this emerging area. We further present our initial effort to address these requirements with PauseNow, an innovative digital wellbeing intervention designed to align users' digital behaviors with their intentions. PauseNow leverages digital nudging and intention-aware recommendations to gently guide users back to their original intentions when they ""get lost"" during their digital usage, promoting a more mindful use of social media.","Fri, 8 Mar 2024 23:26:07 UTC (1,696 KB)"
"128","Mining Issue Trackers: Concepts and Techniques","Lloyd Montgomery, Clara Lüders, Walid Maalej","Software Engineering (cs.SE)","An issue tracker is a software tool used by organisations to interact with users and manage various aspects of the software development lifecycle. With the rise of agile methodologies, issue trackers have become popular in open and closed-source settings alike. Internal and external stakeholders report, manage, and discuss ""issues"", which represent different information such as requirements and maintenance tasks. Issue trackers can quickly become complex ecosystems, with dozens of projects, hundreds of users, thousands of issues, and often millions of issue evolutions. Finding and understanding the relevant issues for the task at hand and keeping an overview becomes difficult with time. Moreover, managing issue workflows for diverse projects becomes more difficult as organisations grow, and more stakeholders get involved. To help address these difficulties, software and requirements engineering research have suggested automated techniques based on mining issue tracking data. Given the vast amount of textual data in issue trackers, many of these techniques leverage natural language processing. This chapter discusses four major use cases for algorithmically analysing issue data to assist stakeholders with the complexity and heterogeneity of information in issue trackers. The chapter is accompanied by a follow-along demonstration package with JupyterNotebooks.","Fri, 8 Mar 2024 23:02:41 UTC (704 KB)"
"129","A VLBI Software Correlator for Fast Radio Transients","Calvin Leung, Shion Andrew, Kiyoshi W. Masui, Charanjot Brar, Tomas Cassanelli, Shami Chatterjee, Victoria Kaspi, Kholoud Khairy, Adam E. Lanman, Mattias Lazda, Juan Mena-Parra, Gavin Noble, Aaron B. Pearlman, Mubdi Rahman, Pranav Sanghavi","Instrumentation and Methods for Astrophysics (astro-ph.IM)","One major goal in fast radio burst science is to detect fast radio bursts (FRBs) over a wide field of view without sacrificing the angular resolution required to pinpoint them to their host galaxies. Wide-field detection and localization capabilities have already been demonstrated using connected-element interferometry; the CHIME/FRB Outriggers project will push this further using widefield cylindrical telescopes as widefield outriggers for very long baseline interferometry (VLBI). This paper describes an offline VLBI software correlator written in Python for the CHIME/FRB Outriggers project. It includes features well-suited to modern widefield instruments like multibeaming/multiple phase center correlation, pulse gating including coherent dedispersion, and a novel correlation algorithm based on the quadratic estimator formalism. This algorithm mitigates sensitivity loss which arises in instruments where the windowing and channelization is done outside the VLBI correlator at each station, which accounts for a 30 percent sensitivity drop away from the phase center. Our correlation algorithm recovers this sensitivity on both simulated and real data. As an end to end check of our software, we have written a preliminary pipeline for VLBI calibration and single-pulse localization, which we use in Lanman et al. (2024) to verify the astrometric accuracy of the CHIME/FRB Outriggers array.","Fri, 8 Mar 2024 19:11:39 UTC (7,856 KB)"
"130","Teranga Go!: Carpooling Collaborative Consumption Community with multi-criteria hesitant fuzzy linguistic term set opinions to build confidence and trust","Rosana Montes, Ana M. Sanchez, Pedro Villar, Francisco Herrera","Computers and Society (cs.CY)","Classic Delphi and Fuzzy Delphi methods are used to test content validity of a data collection tools such as questionnaires. Fuzzy Delphi takes the opinion issued by judges from a linguistic perspective reducing ambiguity in opinions by using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic Delphi method to deal with scenarios in which judges show different expertise degrees by using fuzzy multigranular semantics of the linguistic terms and to obtain intermediate and final results expressed by 2-tuple linguistic values. The key idea of our proposal is to validate the full questionnaire by means of the evaluation of its parts, defining the validity of each item as a Decision Making problem. Taking the opinion of experts, we measure the degree of consensus, the degree of consistency, and the linguistic score of each item, in order to detect those items that affect, positively or negatively, the quality of the instrument. Considering the real need to evaluate a b-learning educational experience with a consensual questionnaire, we present a Decision Making model for questionnaire validation that solve it. Additionally, we contribute to this consensus reaching problem by developing an online tool under GPL v3 license. The software visualizes the collective valuations for each iteration and assists to determine which parts of the questionnaire should be modified to reach a consensual solution.","Wed, 7 Feb 2024 15:50:54 UTC (1,270 KB)"
"131","Explaining Code Examples in Introductory Programming Courses: LLM vs Humans","Arun-Balajiee Lekshmi-Narayanan, Priti Oli, Jeevan Chapagain, Mohammad Hassany, Rabin Banjade, Peter Brusilovsky, Vasile Rus","Computers and Society (cs.CY)","Worked examples, which present an explained code for solving typical programming problems are among the most popular types of learning content in programming classes. Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code. However, instructors rarely have time to provide explanations for many examples typically used in a programming class. In this paper, we assess the feasibility of using LLMs to generate code explanations for passive and active example exploration systems. To achieve this goal, we compare the code explanations generated by chatGPT with the explanations generated by both experts and students.","Sat, 9 Dec 2023 01:06:08 UTC (1,238 KB)[v2] Tue, 12 Mar 2024 02:06:25 UTC (1,238 KB)"
"132","Evaluating AI and Human Authorship Quality in Academic Writing through Physics Essays","Will Yeadon, Elise Agra, Oto-obong Inyang, Paul Mackay, Arin Mizouri","Physics Education (physics.ed-ph)","This study evaluates $n = 300$ short-form physics essay submissions, equally divided between student work submitted before the introduction of ChatGPT and those generated by OpenAI's GPT-4. In blinded evaluations conducted by five independent markers who were unaware of the origin of the essays, we observed no statistically significant differences in scores between essays authored by humans and those produced by AI (p-value $= 0.107$, $\alpha$ = 0.05). Additionally, when the markers subsequently attempted to identify the authorship of the essays on a 4-point Likert scale - from `Definitely AI' to `Definitely Human' - their performance was only marginally better than random chance. This outcome not only underscores the convergence of AI and human authorship quality but also highlights the difficulty of discerning AI-generated content solely through human judgment. Furthermore, the effectiveness of five commercially available software tools for identifying essay authorship was evaluated. Among these, ZeroGPT was the most accurate, achieving a 98% accuracy rate and a precision score of 1.0 when its classifications were reduced to binary outcomes. This result is a source of potential optimism for maintaining assessment integrity. Finally, we propose that texts with $\leq 50\%$ AI-generated content should be considered the upper limit for classification as human-authored, a boundary inclusive of a future with ubiquitous AI assistance whilst also respecting human-authorship.","Fri, 8 Mar 2024 17:10:41 UTC (609 KB)"
"133","On Practicality of Using ARM TrustZone Trusted Execution Environment for Securing Programmable Logic Controllers","Zhiang Li, Daisuke Mashima, Wen Shei Ong, Ertem Esiner, Zbigniew Kalbarczyk, Ee-Chien Chang","Cryptography and Security (cs.CR)","Programmable logic controllers (PLCs) are crucial devices for implementing automated control in various industrial control systems (ICS), such as smart power grids, water treatment systems, manufacturing, and transportation systems. Owing to their importance, PLCs are often the target of cyber attackers that are aiming at disrupting the operation of ICS, including the nation's critical infrastructure, by compromising the integrity of control logic execution. While a wide range of cybersecurity solutions for ICS have been proposed, they cannot counter strong adversaries with a foothold on the PLC devices, which could manipulate memory, I/O interface, or PLC logic itself. These days, many ICS devices in the market, including PLCs, run on ARM-based processors, and there is a promising security technology called ARM TrustZone, to offer a Trusted Execution Environment (TEE) on embedded devices. Envisioning that such a hardware-assisted security feature becomes available for ICS devices in the near future, this paper investigates the application of the ARM TrustZone TEE technology for enhancing the security of PLC. Our aim is to evaluate the feasibility and practicality of the TEE-based PLCs through the proof-of-concept design and implementation using open-source software such as OP-TEE and OpenPLC. Our evaluation assesses the performance and resource consumption in real-world ICS configurations, and based on the results, we discuss bottlenecks in the OP-TEE secure OS towards a large-scale ICS and desired changes for its application on ICS devices. Our implementation is made available to public for further study and research.","Fri, 8 Mar 2024 16:55:20 UTC (3,044 KB)"
"134","Multi-reference coupled cluster theory using the normal ordered exponential ansatz","Alexander Gunasekera, Nicholas Lee, David P. Tew","Chemical Physics (physics.chem-ph)","Properly spin-adapted coupled-cluster theory for general open-shell configurations remains an elusive goal in electronic structure theory. In this contribution we examine Lindgren's normal-ordered exponential ansatz using spin-free excitation operators, with the aid of automatic equation generation software. We present a size-extensive reformulation of the unlinked working equations, and analyse the performance of the method with single and double excitations for simple molecular systems in terms of accuracy and size consistency.","Fri, 8 Mar 2024 15:35:26 UTC (363 KB)"
"135","Scalable Software as a Service Architecture","Ardy Dedase","Software Engineering (cs.SE)","This paper explores the architecture of Software as a Service (SaaS) platforms, emphasizing scalability and maintainability. SaaS, a flexible software distribution model suitable for individuals and organizations, has become prevalent with the advent of Cloud services. This paper aims to provide a high-level design reference for establishing a scalable and maintainable SaaS architecture.","Fri, 8 Mar 2024 15:09:27 UTC (758 KB)"
"136","On a Software Joint Velocity Limitation of a Spherical Parallel Manipulator with Coaxial Input Shafts","Alexandre Lê, Guillaume Rance, Fabrice Rouillier, Arnaud Quadrat, Damien Chablat","Robotics (cs.RO)","This article discusses the implementation of a software joint velocity limitation dedicated to a Spherical Parallel Manipulator (SPM) with coaxial input shafts (CoSPM) using a speed control loop. Such an algorithm takes as input the current joint positions as well as the joint reference velocities computed by the speed controller and limit the latter in order to avoid any known singular configuration. This limitation takes into account the workspace properties of the mechanism and the physical characteristics of its actuators. In particular, one takes advantage of the coaxiality of the input shafts of the CoSPM and the resulting unlimited bearing.","Tue, 5 Mar 2024 14:45:41 UTC (38,566 KB)"
"137","Load Balancing For High Performance Computing Using Quantum Annealing","Omer Rathore, Alastair Basden, Nicholas Chancellor, Halim Kusumaatmaja","Quantum Physics (quant-ph)","With the advent of exascale computing, effective load balancing in massively parallel software applications is critically important for leveraging the full potential of high performance computing systems. Load balancing is the distribution of computational work between available processors. Here, we investigate the application of quantum annealing to load balance two paradigmatic algorithms in high performance computing. Namely, adaptive mesh refinement and smoothed particle hydrodynamics are chosen as representative grid and off-grid target applications. While the methodology for obtaining real simulation data to partition is application specific, the proposed balancing protocol itself remains completely general. In a grid based context, quantum annealing is found to outperform classical methods such as the round robin protocol but lacks a decisive advantage over more advanced methods such as steepest descent or simulated annealing despite remaining competitive. The primary obstacle to scalability is found to be limited coupling on current quantum annealing hardware. However, for the more complex particle formulation, approached as a multi-objective optimization, quantum annealing solutions are demonstrably Pareto dominant to state of the art classical methods across both objectives. This signals a noteworthy advancement in solution quality which can have a large impact on effective CPU usage.","Fri, 8 Mar 2024 12:58:12 UTC (3,103 KB)"
"138","ADROIT6G DAI-driven Open and Programmable Architecture for 6G Networks","Christophoros Christophorou, Iacovos Ioannou, Vasos Vassiliou, Loizos Christofi, John S Vardakas, Erin E Seder, Carla Fabiana Chiasserini, Marius Iordache, Chaouki Ben Issaid, Ioannis Markopoulos, Giulio Franzese, Tanel Järvet, Christos Verikoukis","Networking and Internet Architecture (cs.NI)","In the upcoming 6G era, mobile networks must deal with more challenging applications (e.g., holographic telepresence and immersive communication) and meet far more stringent application requirements stemming along the edge-cloud continuum. These new applications will create an elevated level of expectations on performance, reliability, ubiquity, trustworthiness, security, openness, and sustainability, pushing the boundaries of innovation and driving transformational change across the architecture of future mobile networks. Towards this end, ADROIT6G proposes a set of disruptive innovations with a clear vision on setting a 6G network architecture that can be tailored to the requirements of innovative applications and match the ambitious KPIs set for 6G networks. More specifically, the key transformations that ADROIT6G considers essential to 6G network evolution are: i) AI/ML-powered optimisations across the network, exploring solutions in the ""Distributed Artificial Intelligence (DAI)"" domain for high performance and automation; ii) Transforming to fully cloud-native network software, which can be implemented across various edge-cloud platforms, with security built integrally into the network user plan; and iii) Software driven, zero-touch operations and ultimately automation of every aspect of the network and the services it delivers.","Fri, 8 Mar 2024 12:58:00 UTC (1,166 KB)"
"139","AQuA: Automated Question-Answering in Software Tutorial Videos with Visual Anchors","Saelyne Yang, Jo Vermeulen, George Fitzmaurice, Justin Matejka","Human-Computer Interaction (cs.HC)","Tutorial videos are a popular help source for learning feature-rich software. However, getting quick answers to questions about tutorial videos is difficult. We present an automated approach for responding to tutorial questions. By analyzing 633 questions found in 5,944 video comments, we identified different question types and observed that users frequently described parts of the video in questions. We then asked participants (N=24) to watch tutorial videos and ask questions while annotating the video with relevant visual anchors. Most visual anchors referred to UI elements and the application workspace. Based on these insights, we built AQuA, a pipeline that generates useful answers to questions with visual anchors. We demonstrate this for Fusion 360, showing that we can recognize UI elements in visual anchors and generate answers using GPT-4 augmented with that visual information and software documentation. An evaluation study (N=16) demonstrates that our approach provides better answers than baseline methods.","Fri, 8 Mar 2024 10:55:43 UTC (10,741 KB)"
"140","CommitBench: A Benchmark for Commit Message Generation","Maximilian Schall, Tamara Czinczoll, Gerard de Melo","Computation and Language (cs.CL)","Writing commit messages is a tedious daily task for many software developers, and often remains neglected. Automating this task has the potential to save time while ensuring that messages are informative. A high-quality dataset and an objective benchmark are vital preconditions for solid research and evaluation towards this goal. We show that existing datasets exhibit various problems, such as the quality of the commit selection, small sample sizes, duplicates, privacy issues, and missing licenses for redistribution. This can lead to unusable models and skewed evaluations, where inferior models achieve higher evaluation scores due to biases in the data. We compile a new large-scale dataset, CommitBench, adopting best practices for dataset creation. We sample commits from diverse projects with licenses that permit redistribution and apply our filtering and dataset enhancements to improve the quality of generated commit messages. We use CommitBench to compare existing models and show that other approaches are outperformed by a Transformer model pretrained on source code. We hope to accelerate future research by publishing the source code( this https URL ).","Fri, 8 Mar 2024 09:56:45 UTC (280 KB)"
"141","Using Machine Learning to Separate Cherenkov and Scintillation Light in Hybrid Neutrino Detector","Ayse Bat","Instrumentation and Detectors (physics.ins-det)","This research investigates the separation of Cherenkov and Scintillation light signals within a simulated Water-based Liquid Scintillator (WbLS) detector, utilizing the XGBoost machine learning algorithm. The simulation data were gathered using the Rat-Pac software, which was built on the Geant4 architecture. The use of the WbLS medium has the capability to generate both Scintillation and Cherenkov light inside a single detector. To show the separation power of these two physics events, we will use the supervised learning approach. The assessment utilized a confusion matrix, classification report, and ROC curve, with the ROC curve indicating a performance result of $0.96 \pm 1.2\times 10^{-4}$. The research also aimed to identify essential parameters for effectively distinguishing these physics events through machine learning. For this, the study also introduced the SHAP methodology, utilizing game theory to assess feature contributions. The findings demonstrated that the number of hits has a significant effect on the trained model, while the mean hit time has a somewhat smaller impact. This research advances the utilization of AI and simulation data for accurate Cherenkov and Scintillation light separation in neutrino detectors","Fri, 8 Mar 2024 09:51:09 UTC (1,723 KB)"
"142","Bug Priority Change: An Empirical Study on Apache Projects","Zengyang Li, Guangzong Cai, Qinyi Yu, Peng Liang, Ran Mo, Hui Liu","Software Engineering (cs.SE)","In issue tracking systems, each bug is assigned a priority level (e.g., Blocker, Critical, Major, Minor, or Trivial in JIRA from highest to lowest), which indicates the urgency level of the bug. In this sense, understanding bug priority changes helps to arrange the work schedule of participants reasonably, and facilitates a better analysis and resolution of bugs. According to the data extracted from JIRA deployed by Apache, a proportion of bugs in each project underwent priority changes after such bugs were reported, which brings uncertainty to the bug fixing process. However, there is a lack of indepth investigation on the phenomenon of bug priority changes, which may negatively impact the bug fixing process. Thus, we conducted a quantitative empirical study on bugs with priority changes through analyzing 32 non-trivial Apache open source software projects. The results show that: (1) 8.3% of the bugs in the selected projects underwent priority changes; (2) the median priority change time interval is merely a few days for most (28 out of 32) projects, and half (50. 7%) of bug priority changes occurred before bugs were handled; (3) for all selected projects, 87.9% of the bugs with priority changes underwent only one priority change, most priority changes tend to shift the priority to its adjacent priority, and a higher priority has a greater probability to undergo priority change; (4) bugs that require bug-fixing changes of higher complexity or that have more comments are likely to undergo priority changes; and (5) priorities of bugs reported or allocated by a few specific participants are more likely to be modified, and maximally only one participant in each project tends to modify priorities.","Fri, 8 Mar 2024 05:10:57 UTC (1,708 KB)"
"143","Efficient Calculations for k-diagonal Circulant Matrices and Cyclic Banded Matrices","Chen Wang, Chao Wang","Mathematical Software (cs.MS)","Calculating the inverse of $k$-diagonal circulant matrices and cyclic banded matrices is a more challenging problem than calculating their determinants. Algorithms that directly involve or specify linear or quadratic complexity for the inverses of these two types of matrices are rare. This paper presents two fast algorithms that can compute the complexity of a $k$-diagonal circulant matrix within complexity $O(k^3 \log n+k^4)+kn$, and for $k$-diagonal cyclic banded matrices it is $O(k^3 n+k^5)+kn^2$. Since $k$ is generally much smaller than $n$, the cost of these two algorithms can be approximated as $kn$ and $kn^2$.","Fri, 8 Mar 2024 04:50:04 UTC (252 KB)"
"144","Vertical and radial distribution of atomic carbon in HD 163296","F. Urbina, J. Miley, M. Kama, L. Keyte","Earth and Planetary Astrophysics (astro-ph.EP)","In protoplanetary disks, atomic carbon is expected to originate from the PDR at the disk surface where CO is dissociated by UV photons coming from the stellar, or external interstellar, radiation field. Even though atomic carbon has been detected in several protoplanetary disks, there is a lack of spatially resolved observations of it. For HD 163296 protoplanetary disk, we aim to obtain both radial and vertical structure of [CI]$={^3}{P_1} - {^3}{P_0}$ line emission and perform the first direct comparison of this tracer with optically thick line emission $^{12}$CO $J=2-1$. We used archival ALMA data for [CI]$={^3}{P_1} - {^3}{P_0}$ and previously published ${^{12}}$ CO $J=2-1$ data in HD 163296. Through the software of disksurf we extracted the vertical structure, meanwhile radial profiles were obtained directly from imaging. Brand new DALI modelling was employed to perform direct comparison to the data. We found that these tracers are collocated radially but not vertically, where $^{12}$CO $J=2-1$ emission is, on average, located at higher altitudes, as it is also the case for other tracers in the same disk. Due to this difference in vertical height of the emission, the optically thick $^{12}$CO $J=2-1$ emission line appears to trace the highest altitudes, despite the expected formation mechanism of [CI] in the disk. The latter phenomena may be due to efficient mixing of the upper layers of the disk, or UV photons penetrating deeper than we expected.","Fri, 8 Mar 2024 04:36:08 UTC (3,536 KB)"
"145","collapse: Advanced and Fast Statistical Computing and Data Transformation in R","Sebastian Krantz","Computation (stat.CO)","collapse is a large C/C++-based infrastructure package facilitating complex statistical computing, data transformation, and exploration tasks in R - at outstanding levels of performance and programming efficiency. It also implements a class-agnostic approach to R programming, supporting vector, matrix and data frame-like objects and their popular variants (e.g., factor, ts, xts, tibble, data.table, sf), enabling its seamless integration with large parts of the R ecosystem. This article introduces the package's key components and design principles in a structured way, supported by a rich set of examples. A small benchmark demonstrates its computational performance.","Fri, 8 Mar 2024 04:29:02 UTC (44 KB)"
"146","Effective Fault Localization using Probabilistic and Grouping Approach","Saksham Sahai Srivastava, Arpita Dutta, Rajib Mall","Software Engineering (cs.SE)","Context: Fault localization (FL) is the key activity while debugging a program. Any improvement to this activity leads to significant improvement in total software development cost. There is an internal linkage between the program spectrum and test execution result. Conditional probability in statistics captures the probability of occurring one event in relationship to one or more other events. Objectives: The aim of this paper is to use the conception of conditional probability to design an effective fault localization technique. Methods: In the paper, we present a fault localization technique that derives the association between statement coverage information and test case execution result using condition probability statistics. This association with the failed test case result shows the fault containing the probability of that specific statement. Subsequently, we use a grouping method to refine the obtained statement ranking sequence for better fault localization. Results: We evaluated the effectiveness of proposed method over eleven open-source data sets. Our obtained results show that on average, the proposed CGFL method is 24.56% more effective than other contemporary fault localization methods such as D*, Tarantula, Ochiai, Crosstab, BPNN, RBFNN, DNN, and CNN. Conclusion: We devised an effective fault localization technique by combining the conditional probabilistic method with failed test case execution-based approach. Our experimental evaluation shows our proposed method outperforms the existing fault localization techniques.","Fri, 8 Mar 2024 03:55:09 UTC (1,055 KB)"
"147","Profile of Vulnerability Remediations in Dependencies Using Graph Analysis","Fernando Vera, Palina Pauliuchenka, Ethan Oh, Bai Chien Kao, Louis DiValentin, David A. Bader","Software Engineering (cs.SE)","This research introduces graph analysis methods and a modified Graph Attention Convolutional Neural Network (GAT) to the critical challenge of open source package vulnerability remediation by analyzing control flow graphs to profile breaking changes in applications occurring from dependency upgrades intended to remediate vulnerabilities. Our approach uniquely applies node centrality metrics -- degree, norm, and closeness centrality -- to the GAT model, enabling a detailed examination of package code interactions with a focus on identifying and understanding vulnerable nodes, and when dependency package upgrades will interfere with application workflow. The study's application on a varied dataset reveals an unexpected limited inter-connectivity of vulnerabilities in core code, thus challenging established notions in software security. The results demonstrate the effectiveness of the enhanced GAT model in offering nuanced insights into the relational dynamics of code vulnerabilities, proving its potential in advancing cybersecurity measures. This approach not only aids in the strategic mitigation of vulnerabilities but also lays the groundwork for the development of sophisticated, sustainable monitoring systems for the evaluation of work effort for vulnerability remediation resulting from open source software. The insights gained from this study mark a significant advancement in the field of package vulnerability analysis and cybersecurity.","Fri, 8 Mar 2024 02:01:47 UTC (3,963 KB)"
"148","UI Semantic Group Detection: Grouping UI Elements with Similar Semantics in Mobile Graphical User Interface","Shuhong Xiao, Yunnong Chen, Yaxuan Song, Liuqing Chen, Lingyun Sun, Yankun Zhen, Yanfang Chang","Software Engineering (cs.SE)","Texts, widgets, and images on a UI page do not work separately. Instead, they are partitioned into groups to achieve certain interaction functions or visual information. Existing studies on UI elements grouping mainly focus on a specific single UI-related software engineering task, and their groups vary in appearance and function. In this case, we propose our semantic component groups that pack adjacent text and non-text elements with similar semantics. In contrast to those task-oriented grouping methods, our semantic component group can be adopted for multiple UI-related software tasks, such as retrieving UI perceptual groups, improving code structure for automatic UI-to-code generation, and generating accessibility data for screen readers. To recognize semantic component groups on a UI page, we propose a robust, deep learning-based vision detector, UISCGD, which extends the SOTA deformable-DETR by incorporating UI element color representation and a learned prior on group distribution. The model is trained on our UI screenshots dataset of 1988 mobile GUIs from more than 200 apps in both iOS and Android platforms. The evaluation shows that our UISCGD achieves 6.1\% better than the best baseline algorithm and 5.4 \% better than deformable-DETR in which it is based.","Fri, 8 Mar 2024 01:52:44 UTC (27,293 KB)"
"149","Automating the Information Extraction from Semi-Structured Interview Transcripts","Angelina Parfenova","Computation and Language (cs.CL)","This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of BERT embeddings and HDBSCAN clustering. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.","Thu, 7 Mar 2024 13:53:03 UTC (901 KB)"
"150","Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks","Linyuan Gong, Sida Wang, Mostafa Elhoushi, Alvin Cheung","Computation and Language (cs.CL)","We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at this https URL, and the leaderboard is available at this https URL.","Thu, 7 Mar 2024 05:05:56 UTC (245 KB)"
